{
  "metadata": {
    "name": "EDA",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df \u003d spark.read\n              .option(\"header\", \"true\")\n              .options(Map(\"inferSchema\" -\u003e \"true\", \"delimiter\" -\u003e \"\\t\"))\n              .csv(\"file:///team5/data/LabeledFile.csv\")\ndf.count()\nDF.printSchema() "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "puissance PSS\nage_objet_assuree AGO\nvaleur_venale VV\nvaleur_neuve  VN\nCharge_utile  CU\nusage  USG\nanciennete  ANC\nactivite ACT\nclasse CLS\ndelegation  DLG\nage_client AGE\ncivilite  CIV\nRisky RISKY"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.sql.functions._\n\n// Define column groups (for better organization in output)\nval group1 \u003d Seq(\"puissance\", \"age_objet_assuree\", \"valeur_venale\", \"valeur_neuve\",\"Charge_utile\", \"usage\", \"anciennete\")\nval group2 \u003d Seq( \"activite\", \"classe\", \"delegation\",\"age_client\", \"civilite\", \"Risky\")\n\n// Function to calculate null counts for a group of columns\ndef countNullsForGroup(group: Seq[String], df: org.apache.spark.sql.DataFrame): Unit \u003d {\n  val nullCounts \u003d group.map { colName \u003d\u003e\n    count(when(col(colName).isNull || col(colName) \u003d\u003d\u003d \"\", colName)).alias(colName)\n  }\n  df.select(nullCounts: _*).show(truncate \u003d false)\n}\n\n// Calculate and display null counts for each group\nprintln(\"Group 1 Null Counts:\")\ncountNullsForGroup(group1, DF)\n\nprintln(\"Group 2 Null Counts:\")\ncountNullsForGroup(group2, DF)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Créer une nouvelle DataFrame avec uniquement les colonnes spécifiées\nval newDf \u003d dfwd.select(\n  \"puissance\",\n  \"age_objet_assuree\",\n  \"valeur_venale\",\n  \"valeur_neuve\",\n  \"Charge_utile\",\n  \"usage\",\n  \"anciennete\",\n  \"activite\",\n  \"classe\",\n  \"delegation\",\n  \"age_client\",\n  \"civilite\",\n  \"Risky\"\n)\n\n// Afficher les premières lignes de la nouvelle DataFrame pour vérification\nnewDf.show(100)\nnewDf.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nnewDf.createOrReplaceTempView(\"dataset\")"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT \n    Risky, \n    COUNT(*) AS count,\n    (COUNT(*) * 100.0 / (SELECT COUNT(*) FROM dataset)) AS percentage\nFROM dataset\nGROUP BY Risky\n\nORDER BY count DESC;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.createOrReplaceTempView(\"dataset1\")"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\r\nselect count(*)\r\nfrom\r\n(SELECT \r\n    N_SOUSCRIP, sum(sinistre) sin\r\nFROM dataset1\r\n-- WHERE sinistre \u003e 0\r\nGROUP BY N_SOUSCRIP\r\n-- order by sin desc\r\n)\r\nwhere sin \u003c\u003d 0\r\n"
    }
  ]
}