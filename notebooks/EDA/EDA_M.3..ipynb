{
  "metadata": {
    "name": "EDA_M.3",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "sc.hadoopConfiguration.set(\"fs.defaultFS\", \"hdfs://namenode:9000\")\n        "
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%file\nhdfs dfs -ls /user/majesteye/DS05_INSURANCE_DATASET/input/"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%file\n-ls "
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%file\n\nls /user/majesteye/DS05_INSURANCE_DATASET/input/flatFile.csv\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%file\n\nls /user/majesteye"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval filePath \u003d \"file:///team5/data/flatFile.csv\"\n\n// Verify if the file exists (example for HDFS)\nprintln(\"Verifying file existence...\")\n// Use your cluster\u0027s file system commands or tools to ensure the file is accessible\n\n// Load the file\nval dfff \u003d spark.read\n    .format(\"csv\")               // \"csv\" format works for both CSV and TSV\n    .option(\"header\", \"true\")    // Indicates the file has a header row\n    .option(\"inferSchema\", \"true\") // Infers column data types\n    .option(\"delimiter\", \"\\t\")   // Specify tab as the delimiter\n    .load(filePath)              // Correct and accessible file path\n\n// Inspect the DataFrame\nprintln(\"Schema:\")\ndfff.printSchema()\nprintln(\"Sample Data:\")\ndfff.show(2)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfff.createOrReplaceTempView(\"df\")"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect *\nfrom df\nlimit 10"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect *\nfrom df\nwhere N_SOUSCRIP\u003d\u0027100916\u0027\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval filePath \u003d \"file:///team5/data/flatFile.csv\"\n\n// Load the file\nval dfnd \u003d spark.read\n    .format(\"csv\")               // \"csv\" format works for both CSV and TSV\n    .option(\"header\", \"true\")    // Indicates the file has a header row\n    .option(\"inferSchema\", \"true\") // Infers column data types\n    .option(\"delimiter\", \"\\t\")   // Specify tab as the delimiter\n    .load(filePath)      \n    .distinct()\n\nprintln(\"Schema:\")\ndfnd.printSchema()\n\n              "
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfnd.createOrReplaceTempView(\"dfnd\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect *\nfrom dfnd\nlimit 10"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect *\nfrom dfnd\nwhere N_SOUSCRIP\u003d\u0027100916\u0027\n//ok looks like no full dups"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT COUNT(*) AS row_count\nFROM dfnd\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval partitionCounts \u003d dfnd.rdd.mapPartitions(iter \u003d\u003e Iterator(iter.size.toLong))\nval totalRowCount \u003d partitionCounts.reduce(_ + _)\nprintln(s\"Total number of rows: $totalRowCount\")\n/*Total number of rows: 1631846\npartitionCounts: org.apache.spark.rdd.RDD[Long] \u003d MapPartitionsRDD[18] at mapPartitions at \u003cconsole\u003e:26\ntotalRowCount: Long \u003d 1.631.846 !,other one has 4.105.377*/\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n/*cols for main model:nsouscrip,npolice,nobjass,ageclient,ageobj assu,anct,classe,prime***,sinistre****,valv,valn,civilote,sexe,I owuld argue fract and nfracti are also relevant mais they have issues and same thing for other columns as well so we have to inevitably prepare the main df anyways,obv all have to be restudied to see if it can be taken or if to add more cols and hw to clean them,and then fingers crossed the data makes any sense anyways .\ncols for rule based+unsupervised for the time being let\u0027s focus on the initial making of the rules and studying an validating them using unsupervised models,issue tho\nif sinistre and prime are what I want to work on,what does a negative prime mean?"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfnd.persist() // Default storage level (MEMORY_AND_DISK)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval rowCount \u003d dfnd.count()\nprintln(s\"Total number of rows: $rowCount\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//row count on dfnd\ndfnd.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcp.filter($\"nombre_fractions\" \u003d\u003d\u003d \"2,5\" ).orderBy(asc(\"fractionnement\")).show() //0\u003d\u003e2,5 also 1,0\u003d\u003e2,5"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval dfff \u003d spark.read\n    .format(\"csv\")               // Use \"csv\" for both CSV and TSV files\n    .option(\"header\", \"true\")    // Indicates the file has a header row\n    .option(\"inferSchema\", \"true\") // Infers column data types\n    .option(\"delimiter\", \"\\t\")   // Specify tab as the delimiter\n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/flatFile.csv\")\n\ndfff.printSchema() // Prints the schema of the DataFrame\ndfff.show(2)      // Displays the first 10 rows of the DataFrame\n//is the age an outlier? need to investigate this,is it just that it\u0027ssynthetic data..."
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nval dfcf \u003d spark.read\r\n    .format(\"csv\")               // Use \"csv\" for both CSV and TSV files\r\n    .option(\"header\", \"true\")    // Indicates the file has a header row\r\n    .option(\"inferSchema\", \"true\") // Infers column data types\r\n    .option(\"delimiter\", \"\\t\")   // Specify tab as the delimiter\r\n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/client_features.tsv\")\r\n\r\ndfcf.printSchema() // Prints the schema of the DataFrame\r\ndfcf.show(10)      // Displays the first 10 rows of the DataFrame\r\n//is the age an outlier? need to investigate this,is it just that it\u0027ssynthetic data...we shouldn\u0027t have to keep making assumprions...data quality at first glance seems poor,too many outliers to brush off as being data entry issue,year+anciennete??\r\n\r\nval dfcp \u003d spark.read\r\n    .format(\"csv\")               // Use \"csv\" for both CSV and TSV files\r\n    .option(\"header\", \"true\")    // Indicates the file has a header row\r\n    .option(\"inferSchema\", \"true\") // Infers column data types\r\n    .option(\"delimiter\", \"\\t\")   // Specify tab as the delimiter\r\n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/clients_polices.tsv\")\r\n\r\ndfcp.printSchema() // Prints the schema of the DataFrame\r\ndfcp.show(10)      // Displays the first 10 rows of the DataFrame\r\n\r\n\r\nval dfcs \u003d spark.read\r\n    .format(\"csv\")               // Use \"csv\" for both CSV and TSV files\r\n    .option(\"header\", \"true\")    // Indicates the file has a header row\r\n    .option(\"inferSchema\", \"true\") // Infers column data types\r\n    .option(\"delimiter\", \"\\t\")   // Specify tab as the delimiter\r\n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/clients_sinistre.tsv\")\r\n\r\ndfcs.printSchema() // Prints the schema of the DataFrame\r\ndfcs.show(10)      // Displays the first 10 rows of the DataFrame\r\n\r\nval dfof \u003d spark.read\r\n    .format(\"csv\")               // Use \"csv\" for both CSV and TSV files\r\n    .option(\"header\", \"true\")    // Indicates the file has a header row\r\n    .option(\"inferSchema\", \"true\") // Infers column data types\r\n    .option(\"delimiter\", \"\\t\")   // Specify tab as the delimiter\r\n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/object_features.tsv\")\r\n\r\ndfof.printSchema() // Prints the schema of the DataFrame\r\ndfof.show(10)      // Displays the first 10 rows of the DataFrame\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval dfcp \u003d spark.read\n    .format(\"csv\")               // Use \"csv\" for both CSV and TSV files\n    .option(\"header\", \"true\")    // Indicates the file has a header row\n    .option(\"inferSchema\", \"true\") // Infers column data types\n    .option(\"delimiter\", \"\\t\")   // Specify tab as the delimiter\n    .load(\"/team5/data/clients_polices.tsv\")\n\ndfcp.printSchema() // Prints the schema of the DataFrame\ndfcp.show(10)      // Displays the first 10 rows of the DataFrame\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n## Checking DB Relationships kinda"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcf.count() //res3: Long \u003d 1421778\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcf.select(\"N_SOUSCRIP\").distinct().count() //res4: Long \u003d 369406\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcp.count() //perfect res5: Long \u003d 1632060\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcp.select(\"N_SOUSCRIP\").distinct().count() //perfect res6: Long \u003d 369406\n"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcs.count()//res7: Long \u003d 1433546 hmm,odd but can be explainable,can only be sure once we\u0027re working with df with dropped dups"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcs.select(\"N_SOUSCRIP\").distinct().count() //res8: Long \u003d 369406 oh..should be less,looks like the knowledge we were given abt client NOT appearing in sinistre being cuz he did not have anu accidents is likely wrong :),so client does appear with a sinistre \u003d0(ig?)"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfof.count() //res9: Long \u003d 1433546 same as client lines or more so it checks out,note obj assuré has to belong to one and one N_SOUSCR only (to check,after dropping dups)"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfof.select(\"N_OBJET_ASS\").distinct().count()//\u003e\u003e\u003e than the num of clients which is fine"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfof.select(\"N_SOUSCRIP\").distinct().count() //perfectly normal,res5: Long \u003d 369406"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Quantification of duplicates\n"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dfWithoutDuplicates \u003d dfcf.dropDuplicates()\n// Count the rows after dropping duplicates\nval uniqueRowCount \u003d dfWithoutDuplicates.count()\nval duplicateRowCount\u003ddfcf.coun()-uniqueRowCount\nprintln(s\"Number of duplicate rows in dfcf: $duplicateRowCount\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval repartitionedDF \u003d dfcf.repartition(3) // Specify the number of partitions,3 worked,for some time...\n"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.sql.functions._\n\n// Group by all columns and count occurrences\nval dfcfdupc \u003d repartitionedDF\n  .groupBy(repartitionedDF.columns.map(col): _*) // Group by all columns dynamically\n  .count()\n  .filter(col(\"count\") \u003e 1) // Filter rows that have duplicates\n\n// Calculate total duplicate rows\n\nval dfcfduptotal \u003d dfcfdupc\n  .agg((coalesce(sum(\"count\"), lit(0)) - count(\"*\")).as(\"duplicate_total\")) // Total duplicate occurrences - unique duplicate groups\n  .as[Long]\n  .first()\n\n\n\n// Output the result\nprintln(s\"Number of full duplicate rows in dfcf: $dfcfduptotal\")\n//Number of full duplicate rows in dfcf: 0\n//SO no FULL dups?? NEED to look more into it,and if true,then the dropdup is dropping partial dups,in this case it might be better to get the latest year and theeeen drop partial dups no?"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val partialDupCheck \u003d dfcf.groupBy(\"N_SOUSCRIP\").count().filter(col(\"count\") \u003e 1)\npartialDupCheck.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val selectedColumns \u003d Seq(\r\n  \"anciennete\", \r\n  \"civilite\", \r\n  \"delegation\", \r\n  \"gouvernorat\", \r\n  \"classe\", \r\n  \"Type_renouvellement_police\", \r\n  \"activite\", \r\n  \"sexe\", \r\n  \"direction_regionale\", \r\n  \"centre\"\r\n)\r\n\r\nselectedColumns.foreach { column \u003d\u003e\r\n  println(s\"Unique values in column \u0027$column\u0027:\")\r\n  dfcf.select(column).distinct().collect().foreach(println)\r\n}//type renouv beung P,T is?,is two centers ok? i think maybe yes\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfcf.filter($\"Type_renouvellement_police\" \u003d\u003d\u003d \"P,T\"  ).show() //odd,to be noted"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfcf.filter($\"N_SOUSCRIP\" \u003d\u003d\u003d \"817339\"  ).show() //odd,to be noted"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval numExecutors \u003d spark.sparkContext.getExecutorMemoryStatus.size\nprintln(s\"Number of executors: $numExecutors\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcp.select(\"nombre_fractions\").distinct().show() //??? I meaaaan Ig those are your options eh,but what\u0027s 2,5? two and a half orrrr??? need to check type, ULTIMATELY if these cols aren\u0027t important then it doesn\u0027t even matter if they make sense or not"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcp.select(\"nombre_fractions\")//oh..a string,so that\u0027s not a 2 and half but likely some other explanation?? idk but might n,ot matter anyways \n//interestingly enough tho,as seen afew cells bellow,it only shows up twice,so it could\u0027ve been data entry issue or some plausible explanation"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcp.filter($\"fractionnement\" \u003d\u003d\u003d \"0\"  \u0026\u0026 $\"nombre_fractions\" \u003d\u003d\u003d \"2,5\").orderBy(asc(\"nombre_fractions\")).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcp.filter($\"fractionnement\" \u003d\u003d\u003d \"1\"  \u0026\u0026 $\"nombre_fractions\" \u003d\u003d\u003d \"2,5\").orderBy(asc(\"nombre_fractions\")).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcp.filter($\"fractionnement\" \u003d\u003d\u003d \"0\" ).orderBy(asc(\"nombre_fractions\")).show() //wow,if no fraction then number of fraction should be 5...or other way around 0 should be 1,depends,understanding could be wrong"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nval selectedColumns \u003d Seq(\n  \"fractionnement\", \n  \"nombre_fractions\", \n  \"IsToutRisque\" \n)\n\nselectedColumns.foreach { column \u003d\u003e\n  println(s\"Unique values in column \u0027$column\u0027:\")\n  dfcp.select(column).distinct().collect().foreach(println)\n}//fractio 1,0? nom bre fraction 2,5?"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcp.filter($\"fractionnement\" \u003d\u003d\u003d \"1,0\" ).orderBy(asc(\"nombre_fractions\")).show() //odd,n fraction does not take the value 5 tho,when frac is 1,0"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfcp.filter($\"fractionnement\" \u003d\u003d\u003d \"1,0\" ).orderBy(desc(\"nombre_fractions\")).show() //odd,n fraction does not take the value 5 tho,when frac is 1,0"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcp.filter($\"nombre_fractions\" \u003d\u003d\u003d \"2,5\" ).orderBy(asc(\"fractionnement\")).show() //0\u003d\u003e2,5 also 1,0\u003d\u003e2,5"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcf.orderBy(asc(\"N_SOUSCRIP\")).show(100)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## INSPECTING DFCS\n"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcs.show(10) //no need to check all possible vals per col"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## INSPECTING DFOF\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfof.show(10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//need to make sure the data across the rows is not fluctuating weirdly\ndfof.filter($\"N_OBJET_ASS\" \u003d\u003d\u003d \"718TU100\" ).orderBy(\"year\").show() \n//first thing to note,why does the obj belong to more than one entity?\n//VN fluctuates which SHOULD NOT happen,this is just like the age case,so VN eiother to be mean or dropped\n//for VV cars typically go down in value,this is another class col case,fingers crossed it makes sense,or to be dropped\n//age_obj is good//nope*****it\u0027s not\n//place is another age case,mean+round it or drop\n//charge fluctuates another age case,mean or drop"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfof.orderBy(\"N_OBJET_ASS\").show() //n obj assure does not always have the same form...typos or? no.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//let\u0027s check if it\u0027s there in the other files !7642TU150 \u002726MI0819209\ndfcp.orderBy(\"year\").show() \ndfcs.filter($\"N_OBJET_ASS\" \u003d\u003d\u003d \"\u002726MI0819209\" ).orderBy(\"year\").show() \n//they exist"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//check if obj belongs to only one person as should be\ndfof.orderBy(\"N_SOUSCRIP\").show() //a person with many obj is a thing ofc but will have to think abt how to handle it,if only one year is kept,and then year col is dropped\n"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfof.groupBy(\"N_OBJET_ASS\")\n    .agg(countDistinct(\"N_SOUSCRIP\").alias(\"distinct_clients\"))\n    .filter($\"distinct_clients\" \u003e 1)\n    .orderBy($\"distinct_clients\".desc)\n    .show()\n//yikes,why one obj inssured by so many?"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcp.filter($\"N_OBJET_ASS\" \u003d\u003d\u003d \"MOTOLAAAXKHE\" ).orderBy(\"year\").show() //shouldve done dfof\n/*+----------+----+------------+------------+--------------+----------------+------------------+------------+\n|N_SOUSCRIP|year|    N_POLICE| N_OBJET_ASS|fractionnement|nombre_fractions|             Prime|IsToutRisque|\n+----------+----+------------+------------+--------------+----------------+------------------+------------+\n|    900176|2017|1.01086349E8|MOTOLAAAXKHE|             0|               5| 18.46153846153846|          No|\n|    881763|2017|1.01051544E8|MOTOLAAAXKHE|             0|               5| 86.99092857142858|          No|\n|    897771|2017|1.01082081E8|MOTOLAAAXKHE|             0|               5|27.384615384615383|          No|\n|    860097|2017|1.01065309E8|MOTOLAAAXKHE|             0|               5|61.846153846153854|          No|\n|    332924|2017|1.01069225E8|MOTOLAAAXKHE|             0|               5| 53.23076923076923|          No|\n|    860607|2017|1.01082285E8|MOTOLAAAXKHE|             0|               5| 27.07692307692308|          No|\n|    853658|2017|1.01054127E8|MOTOLAAAXKHE|             0|               5| 82.04495604395605|          No|*/"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfof.filter($\"N_OBJET_ASS\" \u003d\u003d\u003d \"MOTOLAAAXKHE\" ).orderBy(\"N_SOUSCRIP\").show() \n//same obj inssured by many ppl even in the same year...so its not like the owner changed or the policy or anything was updated as an explanation\n/*\n+----------+------------+----+-------------+------------+-----------------+-------------+-------+-----------+-----+---------+-----+-------+-------------------+\n|N_SOUSCRIP| N_OBJET_ASS|year|valeur_venale|valeur_neuve|age_objet_assuree|type_vehicule|energie|carrosserie|place|puissance|usage| marque|       Charge_utile|\n+----------+------------+----+-------------+------------+-----------------+-------------+-------+-----------+-----+---------+-----+-------+-------------------+\n|    151362|MOTOLAAAXKHE|2018|       8899.0|      7511.0|             53.0|         null|     ES|  CABRIOLET|  5.0|     45.0| moto|JIALING|  0.743669559046709|\n|    151362|MOTOLAAAXKHE|2018|       4210.0|      2941.0|             51.0|         null|     ES|  CABRIOLET|  3.0|     26.0| moto|JIALING|0.29122706813319704|\n|    151362|MOTOLAAAXKHE|2021|       4874.0|      4016.0|             51.0|         null|     ES|  CABRIOLET|  3.0|     21.0| moto|JIALING| 0.3976343795347194|\n|    151362|MOTOLAAAXKHE|2022|       3054.0|      5934.0|             51.0|         null|     ES|  CABRIOLET|  3.0|     43.0| moto|JIALING| 0.5876185025360607|\n|    151362|MOTOLAAAXKHE|2017|       5682.0|      7112.0|             51.0|         null|     ES|  CABRIOLET|  4.0|     41.0| moto|JIALING| 0.7042428912324993|\n|    151362|MOTOLAAAXKHE|2022|       2147.0|      4572.0|             51.0|         null|     ES|  CABRIOLET|  2.0|     22.0| moto|JIALING|0.45274106721037366|\n|    173804|MOTOLAAAXKHE|2017|       9060.0|      5798.0|             53.0|           49|     ES|       SOLO|  5.0|     45.0| moto|JIALING|  0.574089188386504|\n|    173804|MOTOLAAAXKHE|2018|       7104.0|      1699.0|             52.0|           49|     ES|       SOLO|  4.0|     43.0| moto|JIALING|0.16822781711151732|\n|    314025|MOTOLAAAXKHE|2021|        149.0|       582.0|             51.0|         null|     ES|  CABRIOLET|  2.0|     48.0| moto|JIALING|0.05771785985647075|\n|    314025|MOTOLAAAXKHE|2021|        262.0|      5698.0|             51.0|         null|     ES|  CABRIOLET|  2.0|     18.0| moto|JIALING|  0.564229725829205|\n|    314025|MOTOLAAAXKHE|2022|       2394.0|      3472.0|             51.0|         null|     ES|  CABRIOLET|  2.0|     44.0| moto|JIALING|0.34382914971879575|\n|    332924|MOTOLAAAXKHE|2018|       1236.0|      6813.0|             51.0|         null|     ES|       null|  2.0|     19.0| moto|JIALING| 0.6745975531093777|\n|    332924|MOTOLAAAXKHE|2017|       7185.0|      9318.0|             52.0|         null|     ES|       null|  4.0|     37.0| moto|JIALING| 0.9226566021540784|\n*/"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\nval selectedColumns \u003d Seq(\r\n  \"energie\", \r\n  \"carrosserie\", \r\n  \"place\", \r\n  \"puissance\", \r\n  \"usage\", \r\n  \"marque\"\r\n)\r\n\r\nselectedColumns.foreach { column \u003d\u003e\r\n  println(s\"Unique values in column \u0027$column\u0027:\")\r\n  dfof.select(column).distinct().collect().foreach(println)\r\n}//CI-4P shoudln\u0027t be in energie no? 00 and 1 meaning? might  not matter if we drop the col\r\n//places has weird vals,but the col might not be significant for risky,just noted so it\u0027s not added to the main MLM as it might introduce noise\r\n//idk abt puissance but seems aight\r\n//usage taxi taxi collect louage etc can be risky,this might be left for the main MLM to consider\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfof.orderBy(desc(\"place\")).show() \n"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfof.orderBy(desc(\"N_OBJET_ASS\")).show() \n"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfof.filter($\"place\" \u003d\u003d\u003d \"0.0\" ).orderBy(\"N_SOUSCRIP\").show() \n"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfcs.select(dfcs.columns.map(c \u003d\u003e sum(when(col(c).isNull, 1).otherwise(0)).alias(c)): _*).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndfof.select(dfof.columns.map(c \u003d\u003e sum(when(col(c).isNull, 1).otherwise(0)).alias(c)): _*).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val duplicateRows \u003d dfof.groupBy(dfof.columns.map(col): _*)\n                        .count()\n                        .filter(\"count \u003e 1\")"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nduplicateRows.show(1)"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    }
  ]
}