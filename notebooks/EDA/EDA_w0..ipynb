{
  "metadata": {
    "name": "EDA_w0",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nls /team5/data"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\npip install pandasql\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# File \nflatFile.csv\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nfrom scipy import stats\r\nimport pandasql as ps\r\nimport sqlite3\r\n\r\n\r\ndf \u003d pd.read_csv(\u0027/team5/data/LabeledFile.csv\u0027, delimiter\u003d\u0027\\t\u0027, low_memory\u003dFalse)  "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Basic overview\n---\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# Basic Dataset Information\nprint(\" DATASET OVERVIEW\")\nprint(f\"Total Rows: {len(df)}\")\nprint(f\"Total Columns: {len(df.columns)}\")\nprint(\"\\nColumn Names:\")\nprint(\", \".join(df.columns))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Dataset info :\n- 4105377 , 31 \n\n# High priority labels : \n- Sinistre\n- age_objet_assuree\n- age_client\n- usage\n- anciennete\n- classe\n- IsToutRisque\n- Type_renouvellement_police\n# To consider: \n- puissance\n- energie\n- valeur_venale\n- valeur_neuve\n- Charge_utile\n- demographic Labels"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf.head(20)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n#  Duplicate Analysis\nduplicates \u003d df.duplicated()\nprint(\"\\n DUPLICATE RECORDS\")\nprint(f\"Total Duplicate Rows: {duplicates.sum()}\")\nif duplicates.sum() \u003e 0:\n    print(\"\\nDuplicate Rows Sample:\")\n    print(df[duplicates].head())\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Duplicates Analysis / Removal\n---"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nprint(f\"Initial Rows: {df.shape[0]}\")\ndf \u003d df.drop_duplicates()\nprint(f\"Rows After Removing Duplicates: {df.shape}\")\n# Before  : 4105377\n# After : 1631846, 31"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "- Total Duplicate Rows: 2473531\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "- Rows before removing dups : 4105377 , After 1631846 \n- we went from 4105377 to - 1631846 \u003d 2473531 dropped \u003d\u003d Total dups rows"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Now checking for duplicates in the PK "
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nduplicates \u003d df.duplicated(subset\u003d[\u0027N_SOUSCRIP\u0027])\nnum_duplicates \u003d duplicates.sum()\nprint(num_duplicates)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "- We\u0027ve got 1262440 dups of N_SOUSCRIP"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nunique_values \u003d df[\u0027N_SOUSCRIP\u0027].unique()\nprint(unique_values)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nunique_values \u003d df[\u0027N_SOUSCRIP\u0027].unique().tolist()\nprint(unique_values)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nduplicates \u003d df.duplicated(subset\u003d[\u0027N_SOUSCRIP\u0027, \u0027year\u0027])\nnum_duplicates \u003d duplicates.sum()\nprint(num_duplicates)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "- we\u0027ve got : 1262440 pk duplicated overall\n- But 402584 pk duplicated per in the same year"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n#  Changing display settings\npd.set_option(\u0027display.max_rows\u0027, None) \npd.set_option(\u0027display.max_columns\u0027, None) \n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# Example : (in this example it seems that the same guy assured another object, so at least it has meaning)\n# We have to remove the ones where the same object is repeated within the same year !\n\ndf[(df[\u0027N_SOUSCRIP\u0027] \u003d\u003d 100681) \u0026 (df[\u0027year\u0027] \u003d\u003d 2018)]"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nduplicates \u003d df.duplicated(subset\u003d[\u0027N_SOUSCRIP\u0027, \u0027year\u0027,\u0027N_OBJET_ASS\u0027])\nnum_duplicates \u003d duplicates.sum()\nprint(num_duplicates)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "- Okay we\u0027ve got 238982 in this case"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf \u003d df.sort_values(by\u003d[\u0027N_SOUSCRIP\u0027, \u0027year\u0027, \u0027IsToutRisque\u0027], ascending\u003d[True, True, False])\ndf \u003d df.drop_duplicates(subset\u003d[\u0027N_SOUSCRIP\u0027, \u0027year\u0027, \u0027N_OBJET_ASS\u0027], keep\u003d\u0027first\u0027)\n# Dropping N_SOUSCRIP duplicates, in the same year, for the same N_OBJET_ASS, where we prioritize IsToutRisque to be yes (to not be eliminated) "
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nduplicates \u003d df.duplicated(subset\u003d[\u0027N_SOUSCRIP\u0027,\u0027year\u0027,\u0027N_OBJET_ASS\u0027])\nnum_duplicates \u003d duplicates.sum()\nprint(num_duplicates)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nduplicates \u003d df.duplicated(subset\u003d[\u0027N_SOUSCRIP\u0027,\u0027year\u0027])\nnum_duplicates \u003d duplicates.sum()\nprint(num_duplicates)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf.shape"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Summary : \nwe\u0027ve only dropped the dups where the same N_SOUSCRIP repeats for the same year and for the same N_OBJET_ASS and changing one attribute randomly while doing so ! Maybe the same N_SOUSCRIP updated his insurrance for that object but I dont buy it, they are dropped \nNow we\u0027re left with : (1392864, 31) in our df\n"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf[(df[\u0027N_SOUSCRIP\u0027] \u003d\u003d 642214) \u0026 (df[\u0027N_OBJET_ASS\u0027] \u003d\u003d \u0027MOTOLRYXCBL0\u0027)]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Okay looks like it is less harmful, at least it makes sense now\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Missing Values Analysis / Handling\n---"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# Missing Value Analysis\n\nmissing_values \u003d df.isnull().sum()\nmissing_percentages \u003d 100 * df.isnull().sum() / len(df)\nmissing_table \u003d pd.concat([missing_values, missing_percentages], axis\u003d1, keys\u003d[\u0027Missing Values\u0027, \u0027Percentage Missing\u0027])\n\nprint(\"\\n3. MISSING VALUES\")\nmissing_data \u003d missing_table[missing_table[\u0027Missing Values\u0027] \u003e 0]\nif len(missing_data) \u003e 0:\n    print(missing_data)\nelse:\n    print(\"No missing values found.\")\n\n# carroserie 58.042996 % of missing values, highly doubt it will be of much use\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# Unique Values in Categorical Columns\ncategorical_cols \u003d df.select_dtypes(include\u003d[\u0027object\u0027, \u0027category\u0027]).columns\nprint(\"\\n5. CATEGORICAL COLUMN UNIQUE VALUES\")\nfor col in categorical_cols:\n    unique_values \u003d df[col].nunique()\n    top_values \u003d df[col].value_counts().head(5)\n    print(f\"\\nColumn: {col}\")\n    print(f\"Total Unique Values: {unique_values}\")\n    print(\"Top 5 Values:\\n\", top_values)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# \u003d\u003d\u003d\u003d\u003d Step 2: Handle Missing Values \u003d\u003d\u003d\u003d\u003d\n# Display missing values percentage\nmissing_info \u003d df.isnull().mean() * 100\nprint(\"Missing Values Percentage:\")\nprint(missing_info[missing_info \u003e 0].sort_values(ascending\u003dFalse))"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# Drop columns with \u003e50% missing values\nthreshold \u003d 50\ndf \u003d df.loc[:, df.isnull().mean() * 100 \u003c threshold]\nprint(f\"Columns After Dropping \u003e{threshold}% Missing Values: {df.shape[1]}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf.shape"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# fill None with Unknown in category columns\ncategorical_cols \u003d df.select_dtypes(include\u003d[\u0027object\u0027]).columns\ndf[categorical_cols] \u003d df[categorical_cols].fillna(\u0027Unknown\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# Fill missing values in numerical columns with median\nnumerical_cols \u003d df.select_dtypes(include\u003d[\u0027number\u0027]).columns\ndf[numerical_cols] \u003d df[numerical_cols].fillna(df[numerical_cols].median())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf[\u0027Type_renouvellement_police\u0027].value_counts(normalize\u003dTrue) * 100\n"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf[\u0027energie\u0027].value_counts(normalize\u003dTrue) * 100\n"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf[\u0027activite\u0027].value_counts(normalize\u003dTrue) * 100\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf[\u0027civilite\u0027].value_counts(normalize\u003dTrue) * 100\n"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf[\u0027sexe\u0027].value_counts(normalize\u003dTrue) * 100\n"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf[\u0027IsToutRisque\u0027].value_counts(normalize\u003dTrue) * 100\n"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf[\u0027classe\u0027].value_counts(normalize\u003dTrue) * 100\n"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf[\u0027puissance\u0027].value_counts(normalize\u003dTrue) * 100\n"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf[\u0027usage\u0027].value_counts(normalize\u003dTrue) * 100\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Summary \n- gender :  M     76.364128 | F     21.944554\n- dominant professions : \n    - EDUCATION_FORMATION                   20.024374\n    - EMPLOYE                               17.437389\n    - PROFESSIONS_MEDICALES                 15.899573\n    - RETRAITE                              14.348228\n- top two energie types : ES       ES       70.417260 |  DI       29.582309\n- Type_renouvellement_police  T      92.145447 | P       7.854409\n- it tout risque : No     84.413984 | Yes    15.586016\n- classe : \n    - 1.0     33.107196\n    - 3.0     17.578502\n    - 8.0     14.827522\n    - 4.0     13.420365\n    - 2.0     12.188269\n    - 5.0      6.103828 \n- puissance : \n    - 3.0    45.446719\n    - 2.0    24.251113\n    - 4.0    18.693140\n    - 5.0     7.032842\n    - 6.0     4.200002 \n- usage  : \n    - moto                3.138715\n    - u1                 15.997450\n    - VP                 78.635458\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Statistics\n---"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n#  Numerical Column Statistics\nnumerical_cols \u003d df.select_dtypes(include\u003d[np.number]).columns\nprint(\"\\n6. NUMERICAL COLUMN STATISTICS\")\nnumerical_stats \u003d df[numerical_cols].describe(percentiles\u003d[.25, .5, .75, .90, .99])\nprint(numerical_stats)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf[\u0027classe\u0027].describe(percentiles\u003d[.25, .5, .75, .90, .99]) "
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf[\u0027classe\u0027].value_counts(normalize\u003dTrue) * 100\n"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf[\u0027Sinistre\u0027].describe(percentiles\u003d[.25, .5, .75, .90, .99])"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nvalue_counts \u003d df[\u0027Sinistre\u0027].value_counts(normalize\u003dTrue) * 100\nprint(value_counts[value_counts \u003e 1])\n"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf[\u0027age_objet_assuree\u0027].describe(percentiles\u003d[.25, .5, .75, .90, .99])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Summary : \n    sinistre : most values are 0.00"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nprint(\"\\n7. POTENTIAL OUTLIER INDICATORS\")\nfor col in numerical_cols:\n    Q1 \u003d df[col].quantile(0.25)\n    Q3 \u003d df[col].quantile(0.75)\n    IQR \u003d Q3 - Q1\n    lower_bound \u003d Q1 - (1.5 * IQR)\n    upper_bound \u003d Q3 + (1.5 * IQR)\n    \n    # Clamp the bounds within the actual data range\n    lower_bound \u003d max(lower_bound, df[col].min())\n    upper_bound \u003d min(upper_bound, df[col].max())\n    \n    outliers \u003d df[(df[col] \u003c lower_bound) | (df[col] \u003e upper_bound)]\n    \n    print(f\"\\nColumn: {col}\")\n    print(f\"Potential Outliers: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n    print(f\"Lower Bound: {lower_bound}\")\n    print(f\"Upper Bound: {upper_bound}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Column: year\nPotential Outliers: 0 (0.00%)\nLower Bound: 2017\nUpper Bound: 2022\n\nColumn: Prime\nPotential Outliers: 49077 (3.52%)\nLower Bound: -232.22470350274736\nUpper Bound: 857.6763373397437\n\nColumn: Sinistre\nPotential Outliers: 85869 (6.16%)\nLower Bound: 0.0\nUpper Bound: 240.29225091568549\n\nColumn: puissance\nPotential Outliers: 63731 (4.58%)\nLower Bound: 1.5\nUpper Bound: 5.5\n\nColumn: Charge_utile\nPotential Outliers: 107031 (7.68%)\nLower Bound: 1.0\nUpper Bound: 1.0\n\nColumn: place\nPotential Outliers: 24512 (1.76%)\nLower Bound: 2.0\nUpper Bound: 2.0\n\nColumn: classe\nPotential Outliers: 16243 (1.17%)\nLower Bound: 0.0\nUpper Bound: 8.5\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " # Anyway\n  sql queries below"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# Create an SQLite connection (in-memory database)\nconn \u003d sqlite3.connect(\":memory:\")  # \":memory:\" creates an in-memory database\n# Save the DataFrame as a SQL table\ndf.to_sql(\"FLAT\", conn, index\u003dFalse, if_exists\u003d\"replace\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\nquery \u003d \u0027\u0027\u0027 SELECT N_SOUSCRIP, COUNT(*) AS count, (COUNT(*) * 100.0 / (SELECT COUNT(*) FROM FLAT)) AS percentage FROM FLAT GROUP BY N_SOUSCRIP ORDER BY count DESC LIMIT 10;\u0027\u0027\u0027\nresult \u003d pd.read_sql_query(query, conn)\nprint(result)"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\nquery \u003d \"SELECT * FROM FLAT WHERE N_SOUSCRIP \u003d 642214 LIMIT 20\"\nresult \u003d pd.read_sql_query(query, conn)\n\nprint(result)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\nquery \u003d \u0027\u0027\u0027SELECT * FROM FLAT WHERE year \u003d 2021 AND N_SOUSCRIP \u003d 642214 \n           \u0027\u0027\u0027\n\nresult \u003d pd.read_sql_query(query, conn)\n\nprint(result)"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nquery \u003d \u0027\u0027\u0027SELECT DISTINCT sexe FROM FLAT \n           \u0027\u0027\u0027\n\nresult \u003d pd.read_sql_query(query, conn)\n\nprint(result)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Z-score:\n Z-score  greater than 3 or less than -3 might be considered an outlier."
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# \u003d\u003d\u003d\u003d\u003d  Handle Outliers \u003d\u003d\u003d\u003d\u003d\nfrom scipy.stats import zscore\n\n# Define a function to remove outliers using Z-score\ndef remove_outliers_zscore(data, threshold\u003d3):\n    z_scores \u003d zscore(data.select_dtypes(include\u003d[\u0027number\u0027]))\n    abs_z_scores \u003d abs(z_scores)\n    return data[(abs_z_scores \u003c threshold).all(axis\u003d1)]\n\nprint(f\"Rows Before Removing Outliers: {df.shape[0]}\")\ndf \u003d remove_outliers_zscore(df)\nprint(f\"Rows After Removing Outliers: {df.shape[0]}\")\n# 339887 AFTER removing outliers"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf.shape"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nshared_path \u003d \u0027/team5/data/w_df.csv\u0027\ndf.to_csv(shared_path, index\u003dFalse) \n#this is the df I saved,I avoided running all the cells,it should be the correct one tho?\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Plots below cause why not"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# \u003d\u003d\u003d\u003d\u003d Step 4: Univariate Analysis \u003d\u003d\u003d\u003d\u003d\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot distributions for numerical columns\nfor col in numerical_cols:\n    plt.figure(figsize\u003d(8, 4))\n    sns.histplot(df[col], kde\u003dTrue, bins\u003d30, color\u003d\u0027blue\u0027)\n    plt.title(f\"Distribution of {col}\")\n    plt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# Plot bar plots for categorical columns\nfor col in categorical_cols:\n    plt.figure(figsize\u003d(10, 5))\n    df[col].value_counts().head(10).plot(kind\u003d\u0027bar\u0027, color\u003d\u0027orange\u0027)\n    plt.title(f\"Top Categories in {col}\")\n    plt.show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# Correlation heatmap\nplt.figure(figsize\u003d(12, 8))\ncorr_matrix \u003d df[numerical_cols].corr()\nsns.heatmap(corr_matrix, annot\u003dTrue, cmap\u003d\u0027coolwarm\u0027, fmt\u003d\u0027.2f\u0027)\nplt.title(\"Correlation Heatmap\")\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n#  Visualization: Violin Plots\ndef create_violin_plots(df, numerical_cols):\n    plt.figure(figsize\u003d(15, 10))\n    for i, col in enumerate(numerical_cols, 1):\n        plt.subplot(len(numerical_cols)//2 + 1, 2, i)\n        sns.violinplot(x\u003ddf[col])\n        plt.title(f\u0027Violin Plot of {col}\u0027)\n    plt.tight_layout()\n    plt.show()\n\ncreate_violin_plots(df, numerical_cols)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Creating a Risky label \n---\n"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# this is purely based on my hypothesis im no domaine experrt\n\ndef classify_risk(row):\n    risk_score \u003d 0\n    \n    # each category with its weight\n    age_risk \u003d {\n        1: 3,    # Lowest age category\n        2: 2,    # Lower-middle age category\n        3: 1,    # Middle age category\n        4: 1.5,  # Upper middle age category\n        5: 2,    # Higher age category\n        6: 3     # Highest age category\n    }\n    risk_score +\u003d age_risk.get(row[\u0027age_client\u0027], 1.5)\n    # if no key is matched in the dict, default value is 1.5\n    \n    # Engine Power Risk\n    puissance_risk \u003d {\n        1: 0.5,  # Lowest power category\n        2: 1,    # Lower-middle power\n        3: 1.5,  # Middle power\n        4: 2,    # Upper-middle power\n        5: 2.5,  # High power\n        6: 3     # Highest power category\n    }\n    risk_score +\u003d puissance_risk.get(row[\u0027puissance\u0027], 1.5)\n    \n    # Market Value Risk\n    valeur_venale_risk \u003d {\n        1: 3,    # Lowest market value\n        2: 2,    # Low market value\n        3: 1.5,  # Medium-low market value\n        4: 1,    # Medium market value\n        5: 0.5,  # Higher market value\n        6: 0.25  # Highest market value\n    }\n    risk_score +\u003d valeur_venale_risk.get(row[\u0027valeur_venale\u0027], 1.5)\n    \n    # Vehicle Age Risk\n    age_objet_risk \u003d {\n        1: 3,    # Very old object\n        2: 2.5,  # Old object\n        3: 2,    # Moderately old\n        4: 1.5,  # Middle-aged\n        5: 1,    # Relatively new\n        6: 0.5   # Very new object\n    }\n    risk_score +\u003d age_objet_risk.get(row[\u0027age_objet_assuree\u0027], 1.5)\n    \n    # Seniority Risk\n    anciennete_risk \u003d {\n        1: 3,    # Very new client\n        2: 2.5,  # Relatively new client\n        3: 2,    # Moderate seniority\n        4: 1.5,  # Good seniority\n        5: 1,    # Long-term client\n        6: 0.5   # Very long-term client\n    }\n    risk_score +\u003d anciennete_risk.get(row[\u0027anciennete\u0027], 1.5)\n    \n\n    # Vehicle Classification Risk\n    classe_risk \u003d {\n        1: 3,    # Highest risk classification\n        2: 2.5,  # High-risk classification\n        3: 2,    # Moderate-high risk\n        4: 1.5,  # Moderate risk\n        5: 1,    # Lower risk\n        6: 0.5   # Lowest risk classification\n    }\n    risk_score +\u003d classe_risk.get(row[\u0027classe\u0027], 1.5)\n    \n    if row[\u0027IsToutRisque\u0027] \u003d\u003d \u0027Yes\u0027:\n        risk_score +\u003d 1\n    \n    if row[\u0027usage\u0027] \u003d\u003d \u0027u1\u0027 : # high risk if it is not for personal use ? / i assume u1 is not pu\n        risk_score +\u003d 1\n    \n    if row[\u0027civilite\u0027] in (\u0027Entreprise\u0027,\u0027Etablissement\u0027,\u0027Org\u0027) : # high risk if it\u0027s entreprise I assume\n        risk_score +\u003d 1\n\n    # if row[\u0027Type_renouvellement_police\u0027] \u003d\u003d  ? # im still not sure who is risky here\n    \n    # if row[\u0027energie\u0027] \u003d\u003d ?\n    \n    #if row[\u0027marque\u0027] \u003d\u003d ?\n    \n    if row[\u0027sexe\u0027] \u003d\u003d \u0027M\u0027 : # i think this scientifically proven\n        risk_score +\u003d 1\n    \n    # sinsitre + charge_utile ?\n\n        # to add :  \n        # Category cols : Usage Risk     # civilite    # Type_renouvellement_police # energie # marque # sexe # IsToutRisque\n        # Numeric cols : # sinistre # charge utile\n        \n# +1 +1 +1 +1\n    if risk_score \u003e\u003d 24:\n        return \u0027Extremely High\u0027\n    elif 19 \u003c\u003d risk_score \u003c 24:\n        return \u0027High\u0027\n    elif 14 \u003c\u003d risk_score \u003c 19:\n        return \u0027Medium-High\u0027\n    elif 11 \u003c\u003d risk_score \u003c 14:\n        return \u0027Medium\u0027\n    elif 8 \u003c\u003d risk_score \u003c 11:\n        return \u0027Low-Medium\u0027\n    else:\n        return \u0027Low\u0027\n\ndf_test \u003d df\ndf_test[\u0027Risk_Category\u0027] \u003d df_test.apply(classify_risk, axis\u003d1)\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf_test.head()"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf_test.to_sql(\"test\", conn, index\u003dFalse, if_exists\u003d\"replace\")"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\nquery \u003d \u0027\u0027\u0027SELECT * FROM test WHERE Risk_Category IN (\u0027Medium-High\u0027,\u0027High\u0027,\u0027Extremely High\u0027)\n          LIMIT 3 \u0027\u0027\u0027\n\nresult \u003d pd.read_sql_query(query, conn)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nprint(result)"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nls /\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\n"
    }
  ]
}