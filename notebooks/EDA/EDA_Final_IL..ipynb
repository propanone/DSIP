{
  "metadata": {
    "name": "EDA_Final_IL",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003ch1\u003e Ilhem Final EDA \u003c/h1\u003e"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%file\nls /user/majesteye/DS05_INSURANCE_DATASET/input"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nsc.hadoopConfiguration.set(\"fs.defaultFS\", \"hdfs://namenode:9000\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003ch1\u003e Exploring data of file 1 : client_features.tsv \u003c/h1\u003e"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%spark\nval DF_CF \u003d spark.read\n    .format(\"csv\")               \n    .option(\"header\", \"true\")    \n    .option(\"inferSchema\", \"true\") // Infers column data types\n    .option(\"delimiter\", \"\\t\")   // Specify tab as the delimiter\n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/client_features.tsv\")\n\nDF_CF.printSchema() // Prints the schema of the DataFrame\nDF_CF.show(10) \nDF_CF.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//Summary of the dataset\nDF_CF.select(\"N_SOUSCRIP\", \"year\", \"age_client\", \"anciennete\", \"civilite\",\"delegation\",\"gouvernorat\").describe().show()\nDF_CF.select( \"classe\",\"Type_renouvellement_police\",\"activite\",\"sexe\",\"direction_regionale\",\"centre\").describe().show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//Identifier les valeurs nulles par colonne\nimport org.apache.spark.sql.functions._\n\nval nullCounts \u003d DF_CF.columns.map { colName \u003d\u003e\n  count(when(col(colName).isNull || col(colName) \u003d\u003d\u003d \"\", colName)).alias(colName)\n}\n\nDF_CF.select(nullCounts: _*).show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//Count distinct values\n\nval distinctCounts \u003d DF_CF.columns.map { colName \u003d\u003e\n  val distinctCount \u003d DF_CF.select(col(colName)).distinct().count()\n  (colName, distinctCount)\n}\n\n\ndistinctCounts.foreach { case (colName, count) \u003d\u003e\n  println(s\"Column \u0027$colName\u0027 has $count distinct values.\")\n}\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nDF_CF.select(\"sexe\").distinct().show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nDF_CF.select(\"civilite\").distinct().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%spark\nDF_CF.select(\"age_client\").distinct().show(400)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003ch3\u003e We notice that the dataset is incoherent and inconsistant:\u003cbr\u003e\n- Lot of null values \u003cbr\u003e\n- The primary key (N_SOUSCRIP, year) is repeated many times \u003cbr\u003e\n- Incoherent data : (sexe with the input (J,CP) and civilite with the input (entreprise, org,etablissement, gov...) \u003cbr\u003e\n- Inconsistant data (age \u003d 128, 124 ) \u003cbr\u003e\nTherefore we need to clean data and delete the duplicates (keep a une client ID with the latest year) \u003cbr\u003e\n\u003c/h3\u003e\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Import necessary libraries\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions.Window\n\n// Replace null values with default or meaningful placeholders\nval cleanedDF \u003d DF_CF\n  .withColumn(\"sexe\", when(col(\"sexe\").isin(\"M\", \"F\"), col(\"sexe\")).otherwise(\"Unknown\"))\n  .withColumn(\"civilite\", when(col(\"civilite\").isin(\"Mr\", \"Mme\"), col(\"civilite\")).otherwise(\"Other\"))\n  .withColumn(\"age_client\", when(col(\"age_client\").between(18, 100), col(\"age_client\")).otherwise(null))\n\n// Remove rows with critical null values (e.g., primary keys)\nval filteredDF \u003d cleanedDF.na.drop(Seq(\"N_SOUSCRIP\", \"year\"))\n\n// Identify and remove duplicates based on client_id and year, keeping the latest year\nval windowSpec \u003d Window.partitionBy(\"N_SOUSCRIP\").orderBy(col(\"year\").desc)\nval DF_CF_Final \u003d filteredDF.withColumn(\"row_num\", row_number().over(windowSpec))\n  .filter(col(\"row_num\") \u003d\u003d\u003d 1)\n  .drop(\"row_num\")\n\n// Show cleaned data\nDF_CF_Final.show()\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nDF_CF_Final.count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003ch1\u003e Exploring data of file 2 : client_polices.tsv \u003c/h1\u003e"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//Load the Second Dataset\nval DF_CP \u003d spark.read\n    .format(\"csv\")                \n    .option(\"header\", \"true\")     \n    .option(\"inferSchema\", \"true\") \n    .option(\"delimiter\", \"\\t\")     \n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/clients_polices.tsv\") \n    \n// Inspect the Schema\nprintln(\"Schema of the second dataset:\")\nDF_CP.printSchema()\n\n//Display a Sample of the Data\nprintln(\"Sample rows from the dataset:\")\nDF_CP.show(10)\n\n//Count values\nprintln(\"Number of rows from the dataset:\")\nDF_CP.count()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//Summary of the dataset\nDF_CP.describe().show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Missing Values per Column\nimport org.apache.spark.sql.functions._\n\nval nullCounts \u003d DF_CP.columns.map { colName \u003d\u003e\n  count(when(col(colName).isNull || col(colName) \u003d\u003d\u003d \"\", colName)).alias(colName)\n}\n\nprintln(\"Count of missing values per column:\")\nDF_CP.select(nullCounts: _*).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Count Distinct Value\nprintln(\"Distinct value counts for each column:\")\nval distinctCounts \u003d DF_CP.columns.map { colName \u003d\u003e\n  val distinctCount \u003d DF_CP.select(col(colName)).distinct().count()\n  (colName, distinctCount)\n}\n\ndistinctCounts.foreach { case (colName, count) \u003d\u003e\n  println(s\"Column \u0027$colName\u0027 has $count distinct values.\")\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\r\n\r\n// Create a count of rows grouped by all columns\r\nval duplicatesDF \u003d DF_CP.groupBy(DF_CP.columns.map(col): _*)\r\n  .count()\r\n  .filter(col(\"count\") \u003e 1) // Filter only duplicates (count \u003e 1)\r\n\r\n// Show duplicate rows\r\nprintln(\"Duplicate rows:\")\r\nduplicatesDF.show(truncate \u003d false)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    }
  ]
}