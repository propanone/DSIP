{
  "metadata": {
    "name": "EDA_F_R",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nls /team5/data/"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nsc.hadoopConfiguration.set(\"fs.defaultFS\", \"hdfs://namenode:9000\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nls /flatFile.csv"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003ch1 align\u003d\"center\"\u003eGeneral Schema\u003c/h1\u003e"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Show and Print \"flatFile.csv\" Schema\nval df \u003d spark.read\n    .format(\"csv\")               \n    .option(\"header\", \"true\")   \n    .option(\"inferSchema\", \"true\") \n    .option(\"delimiter\", \"\\t\") \n    .load(\"file:///team5/data/flatFile.csv\")\n\ndf.printSchema() \ndf.show(10)      "
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Display the number of columns\nval columnCount \u003d df.columns.length\nprintln(s\"Number of columns: $columnCount\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Display the number of rows\nval rowCount \u003d df.count()\nprintln(s\"Number of rows: $rowCount\")"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// For SQL Utilisation\ndf.createOrReplaceTempView(\"df_TBL\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003ch1 align\u003d\"center\"\u003eflatFile_deduplicated\u003c/h1\u003e"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Lecture DF \u003d\u003d\u003d\u003e remove duplicate\nval dfwd \u003d spark.read\n              .option(\"header\", \"true\")\n              .options(Map(\"inferSchema\" -\u003e \"true\", \"delimiter\" -\u003e \"\\t\"))\n              .csv(\"file:///team5/data/flatFile.csv\")\n              .distinct"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Display the number of rows\nval rowCount \u003d dfwd.count()\nprintln(s\"Number of rows: $rowCount\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003ch1 align\u003d\"center\"\u003eEDA Interesting Labels\u003c/h1\u003e"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Create a new DataFrame named df2 with only the relevant columns\nval df2 \u003d df.select(\n  col(\"puissance\"),                // Engine power\n  col(\"valeur_venale\"),            // Current market value of the vehicle\n  col(\"valeur_neuve\"),             // Purchase value of the vehicle\n  col(\"usage\"),                    // Usage of the vehicle\n  col(\"age_objet_assuree\"),        // Age of the insured object\n  col(\"anciennete\"),               // Seniority of the policyholder\n  col(\"age_client\").alias(\"Age_client\"), // Age of the client (alias used if case mismatch)\n  col(\"classe\"),                   // Classification of the vehicle/policy\n  col(\"Type_renouvellement_police\").alias(\"type_renouvellement_police\"), // Renewal type\n  col(\"IsToutRisque\")              // Full risk insurance indicator\n)\n\n// Show the schema and a sample of the data to confirm\ndf2.printSchema()\ndf2.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Import necessary functions from Spark SQL\nimport org.apache.spark.sql.functions._\n\n// Create a DataFrame to calculate the number of missing (null) values for each column in df2\nval missingValuesDF \u003d df2.columns.map { colName \u003d\u003e\n  // Count the number of null or missing values for each column\n  val missingCount \u003d df2.filter(col(colName).isNull).count()\n  (colName, missingCount) // Store the column name and its corresponding missing count\n}.toSeq.toDF(\"Column\", \"MissingCount\") // Convert the results to a DataFrame with column names\n\n// Sort the results by the number of missing values in descending order for better readability\nval sortedMissingValuesDF \u003d missingValuesDF.orderBy(desc(\"MissingCount\"))\n\n// Display the schema of the missing values DataFrame to confirm the structure\nsortedMissingValuesDF.printSchema()\n\n// Show the final results\nsortedMissingValuesDF.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Count the total number of rows in df2\nval totalRows \u003d df2.count()\n\n// Remove duplicate rows and count the number of distinct rows\nval distinctRows \u003d df2.distinct().count()\n\n// Calculate the number of duplicate rows by subtracting distinct rows from total rows\nval duplicateCount \u003d totalRows - distinctRows\n\n// Print the results\nprintln(s\"Total rows in df2: $totalRows\")\nprintln(s\"Distinct rows in df2: $distinctRows\")\nprintln(s\"Duplicate rows in df2: $duplicateCount\")\n\n// Optional: Display duplicate rows, if they exist\nif (duplicateCount \u003e 0) {\n  // Group by all columns and count occurrences of each row\n  val duplicateRowsDF \u003d df2.groupBy(df2.columns.map(col): _*) // Group by all columns\n    .count() // Count occurrences of each unique row\n    .filter(col(\"count\") \u003e 1) // Filter rows where count \u003e 1 (indicating duplicates)\n    .orderBy(desc(\"count\")) // Optionally, sort by the number of duplicates\n\n  // Show the duplicate rows\n  duplicateRowsDF.show(truncate \u003d false)\n} else {\n  println(\"No duplicate rows found in df2.\")\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Remove duplicate rows from df2\nval df2WithoutDuplicates \u003d df2.dropDuplicates()\n\n// Show the number of rows before and after removing duplicates\nprintln(s\"Total rows before removing duplicates: ${df2.count()}\")\nprintln(s\"Total rows after removing duplicates: ${df2WithoutDuplicates.count()}\")\n\n// Optional: Show the DataFrame without duplicates\ndf2WithoutDuplicates.show(truncate \u003d false)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Data Processing and Cleaning Steps:\n\n1. **DataFrame Creation (df2)**: \n   - We created a new DataFrame **`df2`** that contains a selection of relevant columns for classifying clients as \"risky\" or \"non-risky\". These columns include features like **puissance**, **valeur_venale**, **valeur_neuve**, **age_objet_assuree**, **anciennete**, **classe**, **type_renouvellement_police**, **IsToutRisque**, etc.\n\n2. **Handling Missing Values**:\n   - We identified and counted the missing values for each of these important columns using the **`isnull()`** function. This helped us understand where data cleaning is needed.\n\n3. **Removing Duplicates**:\n   - We checked for and removed duplicate rows in **`df2`** by using the **`dropDuplicates()`** function to ensure that the DataFrame only contained unique rows.\n\n   - After removing duplicates, here are the results for **`df2`**:\n     - **Total Rows before removing duplicates**: 4,105,377\n     - **Total Rows after removing duplicates**: 84,369\n     - **Duplicate Count**: 4,021,008\n\n   - This reduction in duplicates ensures the DataFrame contains only relevant, unique entries, making it ready for further analysis.\n\nNote: The DataFrame with duplicates removed is referred to as **`df2WithoutDuplicates`**.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    }
  ]
}