{
  "metadata": {
    "name": "EDA_Ranim_NO",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%file\nls /user/majesteye/DS05_INSURANCE_DATASET/input\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nsc.hadoopConfiguration.set(\"fs.defaultFS\", \"hdfs://namenode:9000\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df \u003d spark.read\n    .format(\"csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .option(\"delimiter\", \",\")\n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/*.csv\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\ndf.createOrReplaceTempView(\"DF_TBL\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Displaying the Data"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.show(20)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Display the number of columns"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val columnCount \u003d df.columns.length\nprintln(s\"Number of columns: $columnCount\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Display the number of rows"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val rowCount \u003d df.count()\nprintln(s\"Number of rows: $rowCount\")"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Valeur Venal Exploration \"VV\""
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Column to analyze\nval columnToAnalyze \u003d \"VV\"\n\n// Count distinct non-null values\nval distinctNonNullValues \u003d df.filter(col(columnToAnalyze).isNotNull).select(columnToAnalyze).distinct().count()\n\n// Count of null values\nval nullCount \u003d df.filter(col(columnToAnalyze).isNull).count()\nval nullPercentage \u003d (nullCount.toDouble / df.count()) * 100\n\n// Count frequencies of distinct non-null values\nval nonNullValues \u003d df.filter(col(columnToAnalyze).isNotNull)\n  .groupBy(columnToAnalyze)\n  .count()\n  .orderBy(col(\"count\").desc)\n\n// Display the results in a structured way\nprintln(s\"Column: $columnToAnalyze\")\nprintln(s\"Distinct Values (excluding nulls): $distinctNonNullValues\")\nprintln(f\"Null Values: $nullCount ($nullPercentage%.2f%%)\")\n\nprintln(\"\\nDistinct Values (excluding nulls) and their Frequencies:\")\nnonNullValues.show(truncate \u003d false)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT \n    VV,\n    COUNT(*) AS count,\n    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) AS percentage\nFROM DF_TBL\nWHERE VV IS NOT NULL\nGROUP BY VV\nORDER BY VV"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Type D\u0027activité Exploration \"ACT\""
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Column to analyze\nval columnToAnalyze \u003d \"ACT\"\n\n// Count distinct non-null values\nval distinctNonNullValues \u003d df.filter(col(columnToAnalyze).isNotNull).select(columnToAnalyze).distinct().count()\n\n// Count of null values\nval nullCount \u003d df.filter(col(columnToAnalyze).isNull).count()\nval nullPercentage \u003d (nullCount.toDouble / df.count()) * 100\n\n// Count frequencies of distinct non-null values\nval nonNullValues \u003d df.filter(col(columnToAnalyze).isNotNull)\n  .groupBy(columnToAnalyze)\n  .count()\n  .orderBy(col(\"count\").desc)\n\n// Display the results in a structured way\nprintln(s\"Column: $columnToAnalyze\")\nprintln(s\"Distinct Values (excluding nulls): $distinctNonNullValues\")\nprintln(f\"Null Values: $nullCount ($nullPercentage%.2f%%)\")\n\nprintln(\"\\nDistinct Values (excluding nulls) and their Frequencies:\")\nnonNullValues.show(numRows \u003d Int.MaxValue, truncate \u003d false) // Display all rows without truncation\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT \n    ACT,\n    COUNT(*) AS count,\n    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) AS percentage\nFROM DF_TBL\nWHERE ACT IS NOT NULL\nGROUP BY ACT\nORDER BY ACT"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Age de souscripteur \"AGE\""
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Column to analyze\nval columnToAnalyze \u003d \"AGE\"\n\n// Count distinct non-null values\nval distinctNonNullValues \u003d df.filter(col(columnToAnalyze).isNotNull).select(columnToAnalyze).distinct().count()\n\n// Count of null values\nval nullCount \u003d df.filter(col(columnToAnalyze).isNull).count()\nval nullPercentage \u003d (nullCount.toDouble / df.count()) * 100\n\n// Count frequencies of distinct non-null values\nval nonNullValues \u003d df.filter(col(columnToAnalyze).isNotNull)\n  .groupBy(columnToAnalyze)\n  .count()\n  .orderBy(col(\"count\").desc)\n\n// Display the results in a structured way\nprintln(s\"Column: $columnToAnalyze\")\nprintln(s\"Distinct Values (excluding nulls): $distinctNonNullValues\")\nprintln(f\"Null Values: $nullCount ($nullPercentage%.2f%%)\")\n\nprintln(\"\\nDistinct Values (excluding nulls) and their Frequencies:\")\nnonNullValues.show(numRows \u003d Int.MaxValue, truncate \u003d false) // Display all rows without truncation\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT \n    AGE,\n    COUNT(*) AS count,\n    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) AS percentage\nFROM DF_TBL\nWHERE AGE IS NOT NULL\nGROUP BY AGE\nORDER BY AGE"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Age de Vehicule Exploration \"AGO\""
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Column to analyze\nval columnToAnalyze \u003d \"AGO\"\n\n// Count distinct non-null values\nval distinctNonNullValues \u003d df.filter(col(columnToAnalyze).isNotNull).select(columnToAnalyze).distinct().count()\n\n// Count of null values\nval nullCount \u003d df.filter(col(columnToAnalyze).isNull).count()\nval nullPercentage \u003d (nullCount.toDouble / df.count()) * 100\n\n// Count frequencies of distinct non-null values\nval nonNullValues \u003d df.filter(col(columnToAnalyze).isNotNull)\n  .groupBy(columnToAnalyze)\n  .count()\n  .orderBy(col(\"count\").desc)\n\n// Display the results in a structured way\nprintln(s\"Column: $columnToAnalyze\")\nprintln(s\"Distinct Values (excluding nulls): $distinctNonNullValues\")\nprintln(f\"Null Values: $nullCount ($nullPercentage%.2f%%)\")\n\nprintln(\"\\nDistinct Values (excluding nulls) and their Frequencies:\")\nnonNullValues.show(numRows \u003d Int.MaxValue, truncate \u003d false) // Display all rows without truncation"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT \n    AGO,\n    COUNT(*) AS count,\n    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) AS percentage\nFROM DF_TBL\nWHERE AGO IS NOT NULL\nGROUP BY AGO\nORDER BY AGO"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Années d\u0027Ancienneté \"ANC\""
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Column to analyze\nval columnToAnalyze \u003d \"ANC\"\n\n// Verify if the column exists\nif (df.columns.contains(columnToAnalyze)) {\n  // Count distinct non-null values\n  val distinctNonNullValues \u003d df.filter(col(columnToAnalyze).isNotNull).select(columnToAnalyze).distinct().count()\n\n  // Count of null values\n  val nullCount \u003d df.filter(col(columnToAnalyze).isNull).count()\n  val nullPercentage \u003d (nullCount.toDouble / df.count()) * 100\n\n  // Count frequencies of distinct non-null values\n  val nonNullValues \u003d df.filter(col(columnToAnalyze).isNotNull)\n    .groupBy(columnToAnalyze)\n    .count()\n    .orderBy(col(\"count\").desc)\n\n  // Display the results in a structured way\n  println(s\"Column: $columnToAnalyze\")\n  println(s\"Distinct Values (excluding nulls): $distinctNonNullValues\")\n  println(f\"Null Values: $nullCount ($nullPercentage%.2f%%)\")\n\n  println(\"\\nDistinct Values (excluding nulls) and their Frequencies:\")\n  nonNullValues.show(numRows \u003d Int.MaxValue, truncate \u003d false) // Show all rows without truncation\n} else {\n  println(s\"Column \u0027$columnToAnalyze\u0027 does not exist in the dataset.\")\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT \n    ANC,\n    COUNT(*) AS count,\n    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) AS percentage\nFROM DF_TBL\nWHERE ANC IS NOT NULL\nGROUP BY ANC\nORDER BY ANC"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Type de Renouvellemnt de Police \"C\""
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Column to analyze\nval columnToAnalyze \u003d \"C\"\n\n// Verify if the column exists\nif (df.columns.contains(columnToAnalyze)) {\n  // Count distinct non-null values\n  val distinctNonNullValues \u003d df.filter(col(columnToAnalyze).isNotNull).select(columnToAnalyze).distinct().count()\n\n  // Count of null values\n  val nullCount \u003d df.filter(col(columnToAnalyze).isNull).count()\n  val nullPercentage \u003d (nullCount.toDouble / df.count()) * 100\n\n  // Count frequencies of distinct non-null values\n  val nonNullValues \u003d df.filter(col(columnToAnalyze).isNotNull)\n    .groupBy(columnToAnalyze)\n    .count()\n    .orderBy(col(\"count\").desc)\n\n  // Display the results in a structured way\n  println(s\"Column: $columnToAnalyze\")\n  println(s\"Distinct Values (excluding nulls): $distinctNonNullValues\")\n  println(f\"Null Values: $nullCount ($nullPercentage%.2f%%)\")\n\n  println(\"\\nDistinct Values (excluding nulls) and their Frequencies:\")\n  nonNullValues.show(numRows \u003d Int.MaxValue, truncate \u003d false) // Show all rows without truncation\n} else {\n  println(s\"Column \u0027$columnToAnalyze\u0027 does not exist in the dataset.\")\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT \n    C,\n    COUNT(*) AS count,\n    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) AS percentage\nFROM DF_TBL\nWHERE C IS NOT NULL\nGROUP BY C\nORDER BY C"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions.Window \n\n// Filter out nulls from VV and RISKY, and analyze the distribution\nval vvRiskyDistribution \u003d df.filter(col(\"VV\").isNotNull \u0026\u0026 (col(\"RISKY\") \u003d\u003d\u003d \"Y\" || col(\"RISKY\") \u003d\u003d\u003d \"N\"))\n  .groupBy(\"VV\", \"RISKY\")\n  .count()\n  .withColumn(\"total_per_vv\", sum(\"count\").over(Window.partitionBy(\"VV\")))\n  .withColumn(\"percentage\", (col(\"count\") / col(\"total_per_vv\")) * 100)\n  .select(\n    col(\"VV\"),\n    col(\"RISKY\"),\n    col(\"count\"),\n    round(col(\"percentage\"), 2).alias(\"percentage\")\n  )\n  .orderBy(\"VV\", \"RISKY\")\n\nvvRiskyDistribution.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\ndf.columns.foreach { column \u003d\u003e\n  val distinctCount \u003d df.filter(col(column).isNotNull).select(column).distinct().count()\n  println(s\"Column: $column, Distinct values (excluding nulls): $distinctCount\")\n}\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val columnsToShow \u003d Seq(\"ACT\", \"CRS\", \"GOV\")\n\ncolumnsToShow.foreach { column \u003d\u003e\n  val distinctValues \u003d df.select(column).filter(col(column).isNotNull).distinct().collect().map(_.get(0))\n  println(s\"Distinct values for column $column: ${distinctValues.mkString(\"[\", \", \", \"]\")}\")\n  println()\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val columnsToShow \u003d Seq(\"VN\", \"AGE\", \"AGO\", \"ANC\", \"C\", \"CIV\", \"CLS\", \"CU\", \"DG\", \"EN\", \"FRC\", \"NFC\", \"PLA\", \"PSS\", \"RISKY\",\n\"SX\", \"USG\", \"VV\",\"DLG\",\"MRQ\",\"CEN\")\ncolumnsToShow.foreach { column \u003d\u003e\n  val distinctValues \u003d df.select(column)\n    .filter(col(column).isNotNull)\n    .distinct()\n    .collect()\n    .map(_.get(0))\n  println(s\"Distinct values for column $column: ${distinctValues.mkString(\"[\", \", \", \"]\")}\")\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Null Value Analysis"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.columns.foreach { column \u003d\u003e\n  val totalRows \u003d df.count()\n  val nullCount \u003d df.filter(col(column).isNull).count()\n  val nullPercentage \u003d (nullCount.toDouble / totalRows) * 100\n  println(s\"Column: $column, Null values: $nullCount ($nullPercentage%)\")\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.groupBy(\"RISKY\").count().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Analyze Tendency (mean, stddev, etc.)"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Numerical Data"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\n\n// List of numeric columns for analysis\nval numericColumns \u003d Seq(\"VN\", \"AGE\", \"AGO\", \"ANC\", \"CLS\", \"CU\", \"FRC\", \"NFC\", \"PLA\", \"PSS\", \"VV\")\n\n// Calculate basic statistics (mean, min, max, etc.) for all numeric columns\ndf.describe(numericColumns: _*).show()\n\n// For each numeric column, calculate mean, standard deviation, min, max, kurtosis, and skewness\nnumericColumns.foreach { column \u003d\u003e\n  \n  // Calculate the mean (average) value for the column\n  val meanValue \u003d df.agg(avg(col(column))).first().get(0)\n  \n  // Calculate the standard deviation for the column\n  val stddevValue \u003d df.agg(stddev(col(column))).first().get(0)\n  \n  // Calculate the minimum value for the column\n  val minValue \u003d df.agg(min(col(column))).first().get(0)\n  \n  // Calculate the maximum value for the column\n  val maxValue \u003d df.agg(max(col(column))).first().get(0)\n  \n  // Calculate the kurtosis (measure of peakedness) for the column\n  val kurtosisValue \u003d df.agg(kurtosis(col(column))).first().get(0)\n  \n  // Calculate the skewness (measure of asymmetry) for the column\n  val skewnessValue \u003d df.agg(skewness(col(column))).first().get(0)\n  \n  // Print out the calculated statistics for each column\n  println(s\"Column: $column, Mean: $meanValue, StdDev: $stddevValue, Min: $minValue, Max: $maxValue, Kurtosis: $kurtosisValue, Skewness: $skewnessValue\")\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Categorial Data"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// List of categorical columns for analysis\nval categoricalColumns \u003d Seq(\"C\", \"CIV\", \"DG\", \"EN\", \"RISKY\", \"SX\", \"USG\", \"CRS\", \"GOV\", \"ACT\")\n\n// For each categorical column, calculate the distinct values and their frequencies\ncategoricalColumns.foreach { column \u003d\u003e\n  \n  // Calculate the distinct values for the column\n  val distinctValues \u003d df.select(column).distinct().collect().map(_.get(0))\n  \n  // Print a header for the column\n  println(s\"\\n----- Distinct values for column: $column -----\")\n  \n  // Print the distinct values for the column\n  println(s\"Distinct values: ${distinctValues.mkString(\"[\", \", \", \"]\")}\\n\")\n  \n  // For each distinct value, count how many times it appears in the column\n  distinctValues.foreach { value \u003d\u003e\n    val valueCount \u003d df.filter(col(column) \u003d\u003d\u003d value).count()\n    println(f\"Value: $value%-20s Count: $valueCount\")\n  }\n  \n  // Add a separator for better readability\n  println(\"\\n\" + \"\u003d\"*50 + \"\\n\")\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    }
  ]
}