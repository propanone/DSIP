{
  "metadata": {
    "name": "eda_w",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nls /team5/data"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\npip install pandasql\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## File : \nflatFile.csv\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nfrom scipy import stats\r\nimport pandasql as ps\r\nimport sqlite3\r\n\r\n\r\ndf \u003d pd.read_csv(\u0027/flatFile.csv\u0027, delimiter\u003d\u0027\\t\u0027, low_memory\u003dFalse)  \r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# Data Inspection Function\nprint(\"\u003d\u003d\u003d\u003d\u003d COMPREHENSIVE DATA INSPECTION \u003d\u003d\u003d\u003d\u003d\\n\")\n\n# 1. Basic Dataset Information\nprint(\"1. DATASET OVERVIEW\")\nprint(f\"Total Rows: {len(df)}\")\nprint(f\"Total Columns: {len(df.columns)}\")\nprint(\"\\nColumn Names:\")\nprint(\", \".join(df.columns))"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf.info()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# 2. Duplicate Analysis\nduplicates \u003d df.duplicated()\nprint(\"\\n2. DUPLICATE RECORDS\")\nprint(f\"Total Duplicate Rows: {duplicates.sum()}\")\nif duplicates.sum() \u003e 0:\n    print(\"\\nDuplicate Rows Sample:\")\n    print(df[duplicates].head())\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# 3. Missing Value Analysis\n\nmissing_values \u003d df.isnull().sum()\nmissing_percentages \u003d 100 * df.isnull().sum() / len(df)\nmissing_table \u003d pd.concat([missing_values, missing_percentages], axis\u003d1, keys\u003d[\u0027Missing Values\u0027, \u0027Percentage Missing\u0027])\n\nprint(\"\\n3. MISSING VALUES\")\nmissing_data \u003d missing_table[missing_table[\u0027Missing Values\u0027] \u003e 0]\nif len(missing_data) \u003e 0:\n    print(missing_data)\nelse:\n    print(\"No missing values found.\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# Unique Values in Categorical Columns\ncategorical_cols \u003d df.select_dtypes(include\u003d[\u0027object\u0027, \u0027category\u0027]).columns\nprint(\"\\n5. CATEGORICAL COLUMN UNIQUE VALUES\")\nfor col in categorical_cols:\n    unique_values \u003d df[col].nunique()\n    top_values \u003d df[col].value_counts().head(5)\n    print(f\"\\nColumn: {col}\")\n    print(f\"Total Unique Values: {unique_values}\")\n    print(\"Top 5 Values:\\n\", top_values)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n#  Numerical Column Statistics\nnumerical_cols \u003d df.select_dtypes(include\u003d[np.number]).columns\nprint(\"\\n6. NUMERICAL COLUMN STATISTICS\")\nnumerical_stats \u003d df[numerical_cols].describe(percentiles\u003d[.25, .5, .75, .90, .99])\nprint(numerical_stats)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n#  Data Range and Outlier Indicators\nprint(\"\\n7. POTENTIAL OUTLIER INDICATORS\")\nfor col in numerical_cols:\n    Q1 \u003d df[col].quantile(0.25)\n    Q3 \u003d df[col].quantile(0.75)\n    IQR \u003d Q3 - Q1\n    lower_bound \u003d Q1 - (1.5 * IQR)\n    upper_bound \u003d Q3 + (1.5 * IQR)\n    outliers \u003d df[(df[col] \u003c lower_bound) | (df[col] \u003e upper_bound)]\n    print(f\"\\nColumn: {col}\")\n    print(f\"Potential Outliers: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n    print(f\"Lower Bound: {lower_bound}\")\n    print(f\"Upper Bound: {upper_bound}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# \u003d\u003d\u003d\u003d\u003d Step 1: Remove Duplicates \u003d\u003d\u003d\u003d\u003d\nprint(f\"Initial Rows: {df.shape[0]}\")\ndf \u003d df.drop_duplicates()\nprint(f\"Rows After Removing Duplicates: {df.shape}\")\n# 1631846, 31"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# Check for duplicate rows\nduplicates_specific \u003d df[df.duplicated(subset\u003d[\u0027N_SOUSCRIP\u0027])]\n\n# Display duplicate rows\nprint(\"Duplicate rows:\")\nprint(duplicates)\n\n# Get the count of duplicate rows\nduplicate_count \u003d df.duplicated().sum()\nprint(f\"Number of duplicate rows: {duplicate_count}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nduplicates \u003d df.duplicated(subset\u003d[\u0027N_SOUSCRIP\u0027])\nnum_duplicates \u003d duplicates.sum()\nprint(num_duplicates)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf.drop_duplicates(subset\u003d[\u0027N_SOUSCRIP\u0027], keep\u003d\u0027last\u0027, inplace\u003dTrue)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nduplicates \u003d df.duplicated(subset\u003d[\u0027N_SOUSCRIP\u0027])\nnum_duplicates \u003d duplicates.sum()\nprint(num_duplicates)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# Create an SQLite connection (in-memory database)\nconn \u003d sqlite3.connect(\":memory:\")  # \":memory:\" creates an in-memory database\n# Save the DataFrame as a SQL table\ndf.to_sql(\"FLAT\", conn, index\u003dFalse, if_exists\u003d\"replace\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# Query the table using SQL\nquery \u003d \"SELECT N_SOUSCRIP, COUNT(*) AS count, (COUNT(*) * 100.0 / (SELECT COUNT(*) FROM FLAT)) AS percentage FROM FLAT GROUP BY N_SOUSCRIP ORDER BY count DESC LIMIT 10;\"\nresult \u003d pd.read_sql_query(query, conn)\n\n# Step 4: Show the result\nprint(result)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# Query the table using SQL\nquery \u003d \"SELECT * FROM FLAT WHERE N_SOUSCRIP \u003d 642214 LIMIT 20\"\nresult \u003d pd.read_sql_query(query, conn)\n\nprint(result)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\nquery \u003d \u0027\u0027\u0027SELECT * FROM FLAT WHERE year \u003d 2021 AND N_SOUSCRIP \u003d 642214 \n           \u0027\u0027\u0027\n\nresult \u003d pd.read_sql_query(query, conn)\n\nprint(result)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf.shape\n#  369406, 31\n# 1631846 - 369406 \u003d 1262440 DROPPED"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# \u003d\u003d\u003d\u003d\u003d Step 2: Handle Missing Values \u003d\u003d\u003d\u003d\u003d\n# Display missing values percentage\nmissing_info \u003d df.isnull().mean() * 100\nprint(\"Missing Values Percentage:\")\nprint(missing_info[missing_info \u003e 0].sort_values(ascending\u003dFalse))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# Drop columns with \u003e50% missing values\nthreshold \u003d 50\ndf \u003d df.loc[:, df.isnull().mean() * 100 \u003c threshold]\nprint(f\"Columns After Dropping \u003e{threshold}% Missing Values: {df.shape[1]}\")\n# carroserie dropped 57% missing values\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# Fill missing values in categorical columns with \u0027Unknown\u0027\ncategorical_cols \u003d df.select_dtypes(include\u003d[\u0027object\u0027]).columns\ndf[categorical_cols] \u003d df[categorical_cols].fillna(\u0027Unknown\u0027)\n# replace missing with Unknown"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# Fill missing values in numerical columns with median\nnumerical_cols \u003d df.select_dtypes(include\u003d[\u0027number\u0027]).columns\ndf[numerical_cols] \u003d df[numerical_cols].fillna(df[numerical_cols].median())\n"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ndf.head()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Z-score:\n Z-score  greater than 3 or less than -3 might be considered an outlier."
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# \u003d\u003d\u003d\u003d\u003d Step 3: Handle Outliers \u003d\u003d\u003d\u003d\u003d\nfrom scipy.stats import zscore\n\n# Define a function to remove outliers using Z-score\ndef remove_outliers_zscore(data, threshold\u003d3):\n    z_scores \u003d zscore(data.select_dtypes(include\u003d[\u0027number\u0027]))\n    abs_z_scores \u003d abs(z_scores)\n    return data[(abs_z_scores \u003c threshold).all(axis\u003d1)]\n\nprint(f\"Rows Before Removing Outliers: {df.shape[0]}\")\ndf \u003d remove_outliers_zscore(df)\nprint(f\"Rows After Removing Outliers: {df.shape[0]}\")\n# 339887 AFTER"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# \u003d\u003d\u003d\u003d\u003d Step 4: Univariate Analysis \u003d\u003d\u003d\u003d\u003d\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot distributions for numerical columns\nfor col in numerical_cols:\n    plt.figure(figsize\u003d(8, 4))\n    sns.histplot(df[col], kde\u003dTrue, bins\u003d30, color\u003d\u0027blue\u0027)\n    plt.title(f\"Distribution of {col}\")\n    plt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# Plot bar plots for categorical columns\nfor col in categorical_cols:\n    plt.figure(figsize\u003d(10, 5))\n    df[col].value_counts().head(10).plot(kind\u003d\u0027bar\u0027, color\u003d\u0027orange\u0027)\n    plt.title(f\"Top Categories in {col}\")\n    plt.show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# Correlation heatmap\nplt.figure(figsize\u003d(12, 8))\ncorr_matrix \u003d df[numerical_cols].corr()\nsns.heatmap(corr_matrix, annot\u003dTrue, cmap\u003d\u0027coolwarm\u0027, fmt\u003d\u0027.2f\u0027)\nplt.title(\"Correlation Heatmap\")\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\ndef check_normality(df, numerical_cols):\n    print(\"Normality Tests (Shapiro-Wilk):\")\n    for col in numerical_cols:\n        stat, p_value \u003d stats.shapiro(df[col])\n        print(f\"{col}: p-value \u003d {p_value}\")\n        print(\"Normally distributed\" if p_value \u003e 0.05 else \"Not normally distributed\")\n\ncheck_normality(df, numerical_cols)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# Pairplot for key numerical variables\ndef create_pairplot(df, numerical_cols, sample_size\u003d5000):\n    plt.figure(figsize\u003d(15, 10))\n    sample_df \u003d df.sample(min(len(df), sample_size))\n    sns.pairplot(sample_df[numerical_cols], diag_kind\u003d\u0027kde\u0027)\n    plt.suptitle(\"Pairplot of Numerical Variables\", y\u003d1.02)\n    plt.show()\ncreate_pairplot(df, numerical_cols)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n#  Visualization: Violin Plots\ndef create_violin_plots(df, numerical_cols):\n    plt.figure(figsize\u003d(15, 10))\n    for i, col in enumerate(numerical_cols, 1):\n        plt.subplot(len(numerical_cols)//2 + 1, 2, i)\n        sns.violinplot(x\u003ddf[col])\n        plt.title(f\u0027Violin Plot of {col}\u0027)\n    plt.tight_layout()\n    plt.show()\n\ncreate_violin_plots(df, numerical_cols)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%python\n"
    }
  ]
}