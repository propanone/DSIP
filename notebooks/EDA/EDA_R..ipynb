{
  "metadata": {
    "name": "EDA_R",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%file\nls /user/majesteye/DS05_INSURANCE_DATASET/input"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nsc.hadoopConfiguration.set(\"fs.defaultFS\", \"hdfs://namenode:9000\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003ch1 align\u003d\"center\"\u003eGeneral Schema\u003c/h1\u003e"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Show and Print \"clients_polices\" Schema\nval dfcp \u003d spark.read\n    .format(\"csv\")               \n    .option(\"header\", \"true\")   \n    .option(\"inferSchema\", \"true\") \n    .option(\"delimiter\", \"\\t\") \n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/clients_polices.tsv\")\n\ndfcp.printSchema() \ndfcp.show(10)      \n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Show and Print \"client_features\" Schema\nval dfcf \u003d spark.read\n    .format(\"csv\")               \n    .option(\"header\", \"true\")   \n    .option(\"inferSchema\", \"true\") \n    .option(\"delimiter\", \"\\t\") \n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/client_features.tsv\")\n\ndfcf.printSchema() \ndfcf.show(10)      "
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Show and Print \"clients_sinistre\" Schema\n\nval dfcs \u003d spark.read\n    .format(\"csv\")               \n    .option(\"header\", \"true\")    \n    .option(\"inferSchema\", \"true\") \n    .option(\"delimiter\", \"\\t\")   \n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/clients_sinistre.tsv\")\n\ndfcs.printSchema() \ndfcs.show(10)      "
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Show and Print \"object_features\" Schema\n\nval dfof \u003d spark.read\n    .format(\"csv\")               \n    .option(\"header\", \"true\")    \n    .option(\"inferSchema\", \"true\") \n    .option(\"delimiter\", \"\\t\")   \n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/object_features.tsv\")\n\ndfof.printSchema() \ndfof.show(10)      "
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// For SQL Utilisation\ndfcp.createOrReplaceTempView(\"dfcp_TBL\")\ndfcf.createOrReplaceTempView(\"dfcf_TBL\")\ndfcs.createOrReplaceTempView(\"dfcs_TBL\")\ndfof.createOrReplaceTempView(\"dfof_TBL\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003ch1 align\u003d\"center\"\u003eGeneral Exploration of Each File\u003c/h1\u003e\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003ch1 align\u003d\"center\"\u003e\"clients_polices\" File\u003c/h1\u003e\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## --- Distribution of IsToutRisque and fractionnement ---"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n-- Distribution of `IsToutRisque`\n-- Count occurrences and calculate percentages for each value in the column\nSELECT IsToutRisque, COUNT(*) AS count,\n       COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () AS percentage\nFROM dfcp_TBL\nGROUP BY IsToutRisque;\n\n-- Distribution of `fractionnement`\nSELECT fractionnement, COUNT(*) AS count,\n       COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () AS percentage\nFROM dfcp_TBL\nGROUP BY fractionnement;\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## --- Analysis of the distribution of Prime ---"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Calculate descriptive statistics for the `Prime` column\ndfcp.selectExpr(\n  \"min(Prime) as Min_Prime\",   // Minimum value\n  \"max(Prime) as Max_Prime\",   // Maximum value\n  \"avg(Prime) as Avg_Prime\",   // Average (mean)\n  \"stddev(Prime) as StdDev_Prime\" // Standard deviation (élévé que la moyenne, grande hétérogénéité dans les montants)\n).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Total number of rows in the dataset\nval totalRows \u003d dfcp.count()\n\n// Count rows where `Prime` is negative\nval negativePrimeCount \u003d dfcp.filter(\"Prime \u003c 0\").count()\n\n// Calculate the percentage of negative `Prime` values\nval negativePercentage \u003d (negativePrimeCount.toDouble / totalRows) * 100\n\n// Display the result\nprintln(f\"Number of negative Prime values: $negativePrimeCount\")\nprintln(f\"Percentage of negative Prime values: $negativePercentage%.2f%%\")\n\ndfcp.select(\"Prime\").summary(\"count\", \"min\", \"25%\", \"50%\", \"75%\", \"max\").show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ---Finding duplicates ---"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Group by all columns and count the occurrences\nval duplicateRows \u003d dfcp.groupBy(dfcp.columns.map(col): _*) // Group by all columns\n                        .count()\n                        .filter(\"count \u003e 1\") // Keep only duplicates\n\n// Show duplicate rows with their counts\nduplicateRows.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ---Missing values of dfcp ---"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003ch1 align\u003d\"center\"\u003e\"client_features\" File\u003c/h1\u003e"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Total number of rows in the dataset\nval totalRows \u003d dfcf.count()\n\n// Calculate the number and percentage of missing (null or NaN) values for each column\nval missingValues \u003d dfcf.columns.map { colName \u003d\u003e\n  val missingCount \u003d dfcf.filter(col(colName).isNull || isnan(col(colName))).count() // Count missing values\n  val missingPercentage \u003d (missingCount.toDouble / totalRows) * 100 // Calculate percentage\n  (colName, missingCount, f\"${missingPercentage}%.2f\") // Tuple: column name, missing count, percentage\n}\n\n// Convert the result to a DataFrame for better visualization\nval missingValuesDF \u003d spark.createDataFrame(missingValues)\n                           .toDF(\"Column\", \"Missing Count\", \"Missing Percentage\")\n\n// Show the result\nmissingValuesDF.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dfof \u003d spark.read\n    .format(\"csv\")               \n    .option(\"header\", \"true\")    \n    .option(\"inferSchema\", \"true\") \n    .option(\"delimiter\", \"\\t\")   \n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/object_features.tsv\")\n\ndfof.printSchema() \ndfof.show()      "
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\ndef missingValuesStats(df: org.apache.spark.sql.DataFrame, dfName: String): Unit \u003d {\n  val totalRows \u003d df.count()\n\n  // Calcul des valeurs manquantes par colonne\n  val missingCount \u003d df.columns.map { colName \u003d\u003e\n    val nullCount \u003d df.filter(col(colName).isNull || col(colName) \u003d\u003d\u003d \"\").count()\n    (colName, nullCount)\n  }.toMap\n\n  // Calcul du pourcentage\n  val missingPercentage \u003d missingCount.map {\n    case (colName, count) \u003d\u003e (colName, (count.toDouble / totalRows) * 100)\n  }\n\n  // Affichage des résultats\n  println(s\"\\n\u003d\u003d\u003d Résultats pour $dfName \u003d\u003d\u003d\")\n  missingCount.foreach { case (colName, count) \u003d\u003e\n    val percentage \u003d missingPercentage(colName)\n    println(f\"Colonne: $colName, Valeurs manquantes: $count, Pourcentage: $percentage%.2f%%\")\n  }\n}\n\n// Appliquer la fonction à chaque dataframe\nval dfList \u003d Map( \"dfcf\" -\u003e dfcf, \"dfcp\" -\u003e dfcp, \"dfcs\" -\u003e dfcs, \"dfof\" -\u003e dfof)\ndfList.foreach { case (name, dataframe) \u003d\u003e\n  missingValuesStats(dataframe, name)\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003ch1 align\u003d\"center\"\u003eJoin Tables\u003c/h1\u003e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003cdiv style\u003d\"border: 1px solid #d0e7f5; padding: 10px; background-color: #eef7fc; border-radius: 5px;\"\u003e\n  \u003cp\u003e\u003cstrong\u003eImportant Note:\u003c/strong\u003e\u003c/p\u003e\n  \u003cp\u003e\n    To perform the table joins required for creating the final dataset, we encountered an issue of \n    \u003cstrong\u003enon-unique primary key values\u003c/strong\u003e for the combination of \u003ccode\u003e(N_souscrip, year)\u003c/code\u003e. \n    This caused a conflict during the join process. To resolve this, we decided to remove duplicates \n    by retaining \u003cstrong\u003eonly one record per client for each year\u003c/strong\u003e.\n  \u003c/p\u003e\n\u003c/div\u003e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003ch1 align\u003d\"center\"\u003eSolve the non-unique key issue\u003c/h1\u003e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## For clients-polices "
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions.Window\n\n// Concept:\n// In this code, we will remove duplicates from a DataFrame by keeping only the rows where \"IsToutRisque\" equals \"Yes\"\n// for each unique combination of \"N_SOUSCRIP\" and \"year\". If multiple rows meet this condition, one of them will be kept randomly.\n\ndef removeDuplicatesBasedOnIsToutRisque(df: org.apache.spark.sql.DataFrame): org.apache.spark.sql.DataFrame \u003d {\n  // Repartition the DataFrame by \"N_SOUSCRIP\" to optimize memory usage and shuffle\n  val repartitionedDF \u003d df.repartition(200, col(\"N_SOUSCRIP\"))\n\n  // Remove duplicates by keeping rows where \"IsToutRisque\" equals \"Yes\"\n  val deduplicatedDF \u003d repartitionedDF\n    .withColumn(\"IsToutRisqueNumeric\", when(col(\"IsToutRisque\") \u003d\u003d\u003d \"Yes\", 1).otherwise(0)) // Convert \"IsToutRisque\" to a numeric column for sorting\n    .withColumn(\"Rank\", row_number()\n      .over(Window.partitionBy(\"N_SOUSCRIP\", \"year\") // Partition by \"N_SOUSCRIP\" and \"year\"\n        .orderBy(rand())) // Randomize the order to select a random row when \"IsToutRisque\" is \"Yes\"\n    )\n    .filter(col(\"Rank\") \u003d\u003d\u003d 1) // Keep only the row with rank 1, i.e., the row with \"IsToutRisque\" equal to \"Yes\"\n    .drop(\"IsToutRisqueNumeric\", \"Rank\") // Drop the temporary columns used for sorting and ranking\n\n  deduplicatedDF // Return the deduplicated DataFrame\n}\n\n// Apply the function to the \"dfcp2\" DataFrame and show the results\nval dfcp2 \u003d removeDuplicatesBasedOnIsToutRisque(dfcp) \n\n// Show the first 100 rows of the deduplicated DataFrame\ndfcp2.show(100, truncate \u003d false)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Group by year and N_SOUSCRIP and count the occurrences\nval duplicateCounts \u003d dfcp2\n  .groupBy(\"year\", \"N_SOUSCRIP\")\n  .count()\n  .filter($\"count\" \u003e 1) // Filter only combinations that appear more than once\n\n// Count the number of duplicate combinations\nval numberOfDuplicates \u003d duplicateCounts.count()\n\n// Show the duplicates (if any)\nprintln(s\"Number of duplicate combinations of \u0027year\u0027 and \u0027N_SOUSCRIP\u0027: $numberOfDuplicates\")\nprintln(\"Details of duplicate combinations (if any):\")\nduplicateCounts.show(100) // Display the first 100 duplicate combinations (if any)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## For clients-features "
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Finding duplicates\n// Group by all columns and count the occurrences\nval duplicateRows \u003d dfof.groupBy(dfof.columns.map(col): _*) // Group by all columns\n                        .count()\n                        .filter(\"count \u003e 1\") // Keep only duplicates\n\n// Show duplicate rows with their counts\nduplicateRows.show(10)"
    }
  ]
}