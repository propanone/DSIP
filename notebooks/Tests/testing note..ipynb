{
  "metadata": {
    "name": "testing note",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%file\r\n\r\nls /user/majesteye/DS05_INSURANCE_DATASET/input"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nsc.hadoopConfiguration.set(\"fs.defaultFS\", \"hdfs://namenode:9000\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.storage.StorageLevel"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Lecture des fichiers TSV\nval clientFeaturesDF \u003d spark.read\n    .format(\"csv\") // Pour TSV, utilisez toujours le format \"csv\"\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .option(\"delimiter\", \"\\t\") // Délimiteur tab pour .tsv\n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/client_features.tsv\")\n\nval clientsPolicesDF \u003d spark.read\n    .format(\"csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .option(\"delimiter\", \"\\t\") // Délimiteur tab pour .tsv\n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/clients_polices.tsv\")\n\nval clientsSinistreDF \u003d spark.read\n    .format(\"csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .option(\"delimiter\", \"\\t\") // Délimiteur tab pour .tsv\n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/clients_sinistre.tsv\")\n\n// Optionnel : Afficher un échantillon pour vérifier\nclientFeaturesDF.show()\nclientsPolicesDF.show()\nclientsSinistreDF.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nclientFeaturesDF.persist(StorageLevel.MEMORY_AND_DISK)\nclientsPolicesDF.persist(StorageLevel.MEMORY_AND_DISK)\nclientsSinistreDF.persist(StorageLevel.MEMORY_AND_DISK)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\n// Step 1: Join clientFeaturesDF and clientsPolicesDF\r\nval joinedDF1 \u003d clientFeaturesDF\r\n    .join(clientsPolicesDF, Seq(\"N_SOUSCRIP\", \"year\"), \"outer\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\njoinedDF1.persist(StorageLevel.MEMORY_AND_DISK)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\njoinedDF1.select(\"N_SOUSCRIP\", \"year\", \"N_POLICE\", \"Prime\", \"IsToutRisque\").show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Step 2: Join the result with clientsSinistreDF\nval finalDF \u003d joinedDF1.select(\"N_SOUSCRIP\", \"year\", \"N_POLICE\", \"Prime\", \"IsToutRisque\")\n    .join(clientsSinistreDF, Seq(\"N_SOUSCRIP\", \"year\", \"N_POLICE\"), \"outer\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n// Step 3: Show a sample of the final dataset\nfinalDF.select(\"N_SOUSCRIP\", \"year\", \"IsToutRisque\", \"N_POLICE\").show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions.Window\n\n// Step 1: Join clientFeaturesDF and clientsPolicesDF\nval joinedDF1 \u003d clientFeaturesDF\n    .join(clientsPolicesDF, Seq(\"N_SOUSCRIP\", \"year\"), \"outer\")\n\n// Step 2: Join the result with clientsSinistreDF\nval finalDF \u003d joinedDF1\n    .join(clientsSinistreDF, Seq(\"N_SOUSCRIP\", \"year\", \"N_POLICE\"), \"outer\")\n\n// Step 3: Use Window function to get the row with the maximum year for each N_SOUSCRIP\nval windowSpec \u003d Window.partitionBy(\"N_SOUSCRIP\").orderBy(col(\"year\").desc)\n\n// Step 4: Add a column to mark the row with the highest year for each N_SOUSCRIP\nval finalWithMaxYearDF \u003d finalDF\n    .withColumn(\"rank\", rank().over(windowSpec))\n    .filter(col(\"rank\") \u003d\u003d\u003d 1) // Keep only the rows with rank 1 (the highest year)\n\n// Step 5: Drop the rank column, if needed\nval cleanedDF \u003d finalWithMaxYearDF.drop(\"rank\")\n\n// Now cleanedDF contains only the rows with the highest year for each N_SOUSCRIP\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ncleanedDF.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    }
  ]
}