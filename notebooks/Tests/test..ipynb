{
  "metadata": {
    "name": "test",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%file\nls /user/majesteye/DS05_INSURANCE_DATASET/input\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nsc.hadoopConfiguration.set(\"fs.defaultFS\", \"hdfs://namenode:9000\") \n\nval df \u003d spark.read\n    .format(\"csv\")               // Use \"csv\" for both CSV and TSV files\n    .option(\"header\", \"true\")    // Indicates the file has a header row\n    .option(\"inferSchema\", \"true\") // Infers column data types\n    .option(\"delimiter\", \"\\t\")   // Specify tab as the delimiter\n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/flatFile.csv\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\ndf.createOrReplaceTempView(\"FLAT\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT * FROM FLAT\nLIMIT 5;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nprint(\"hello\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val flat_table \u003d spark.read\n    .format(\"csv\")               \n    .option(\"header\", \"true\")   \n    .option(\"inferSchema\", \"true\") \n    .option(\"delimiter\", \"\\t\") \n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/flatFile.csv\")\n    \nflat_table.createOrReplaceTempView(\"flat\")"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect * \nfrom flat\nlimit 10\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect * from flat\nwhere N_SOUSCRIP\u003d100018"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// shape of the dataset \r\nval rowCount \u003d flat_table.count()\r\nprintln(s\"The dataset has $rowCount rows\")"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// number of null values for each column. to see how to handle this..\n// we can see that we have:\n// 2 in N_POLICE\n// 4 in N_OBJET_ASS\n// 227 in \n\nimport org.apache.spark.sql.functions._\n\ndf_clients_polices.select(df_clients_polices.columns.map(c \u003d\u003e sum(col(c).isNull.cast(\"int\")).alias(c)): _*)\n    .show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// see the proportion of IsToutRisque variable\r\nimport org.apache.spark.sql.functions._\r\n\r\nval valueCounts \u003d df_clients_polices.groupBy(\"IsToutRisque\")\r\n    .count()\r\n\r\nval proportions \u003d valueCounts.withColumn(\"proportion\", round(col(\"count\") / df_clients_polices.count(), 2))\r\n\r\nproportions.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df_clients_polices.createOrReplaceTempView(\"clients_polices\")"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\r\nSELECT IsToutRisque, \r\n           COUNT(*) AS count, \r\n           ROUND(COUNT(*) / (SELECT COUNT(*) FROM clients_polices), 2) AS proportion\r\n    FROM clients_polices\r\n    GROUP BY IsToutRisque\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.createOrReplaceTempView(\"POLICIES_\")"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df_client_features \u003d spark.read\n    .format(\"csv\")               \n    .option(\"header\", \"true\")   \n    .option(\"inferSchema\", \"true\") \n    .option(\"delimiter\", \"\\t\") \n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/client_features.tsv\")\n\ndf_client_features.printSchema() \ndf_client_features.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df_clients_sinistre \u003d spark.read\n    .format(\"csv\")               \n    .option(\"header\", \"true\")   \n    .option(\"inferSchema\", \"true\") \n    .option(\"delimiter\", \"\\t\") \n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/clients_sinistre.tsv\")\n\ndf_clients_sinistre.printSchema()\ndf_clients_sinistre.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df_clients_sinistre.createOrReplaceTempView(\"clients_sinistre\")"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val result \u003d spark.sql(\"\"\"\r\n    SELECT N_SOUSCRIP, COUNT(CASE WHEN Sinistre \u003e 0 THEN 1 END) AS num_sinistre\r\n    FROM CLIENT_SINISTRES\r\n    GROUP BY N_SOUSCRIP\r\n    ORDER BY num_sinistre desc\r\n\"\"\")\r\n\r\nresult.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val result \u003d spark.sql(\"\"\"\r\n    SELECT N_SOUSCRIP, COUNT(CASE WHEN Sinistre \u003e 0 THEN 1 END) AS num_sinistre\r\n    FROM CLIENT_SINISTRES\r\n    GROUP BY N_SOUSCRIP\r\n    ORDER BY num_sinistre desc\r\n\"\"\")\r\n\r\nresult.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val result \u003d spark.sql(\"\"\"\r\n    SELECT N_SOUSCRIP, COUNT(CASE WHEN Sinistre \u003e 0 THEN 1 END) AS num_sinistre\r\n    FROM CLIENT_SINISTRES\r\n    GROUP BY N_SOUSCRIP\r\n    ORDER BY num_sinistre desc\r\n\"\"\")\r\n\r\nresult.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nselect count(*), count(distinct(N_SOUSCRIP))\nfrom CLIENT_SINISTRES"
    }
  ]
}