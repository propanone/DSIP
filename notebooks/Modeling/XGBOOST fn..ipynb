{
  "metadata": {
    "name": "XGBOOST fn",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val labeledDf \u003d spark.read\n              .option(\"header\", \"true\")\n              .options(Map(\"inferSchema\" -\u003e \"true\", \"delimiter\" -\u003e \"\\t\"))\n              .csv(\"file:///team5/data/LabeledFile.csv\")"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nimport xgboost\nprint(xgboost.__version__)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nprint(pickle.format_version)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\npip freeze | grep scikit-learn\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\npip freeze \u003e /team5/data/requirements.txt\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nls /team5/data/models"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nimport pandas as pd\n\n# Load the labeled dataset\nfile_path \u003d \u0027file:///team5/data/LabeledFile.csv\u0027\nlabeled_data \u003d pd.read_csv(file_path,delimiter\u003d\u0027\\t\u0027)\n\n# Display the first few rows to confirm\nprint(labeled_data.head())\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# Filter lines where risky \u003d yes\nrisky_data \u003d labeled_data[labeled_data[\u0027Risky\u0027] \u003d\u003d \u0027yes\u0027]\nprint(risky_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nlabeled_data \u003d labeled_data.sort_values(by\u003d[\u0027N_SOUSCRIP\u0027, \u0027year\u0027, \u0027Risky\u0027], ascending\u003d[True, True, False])\nlabeled_data \u003d labeled_data.drop_duplicates(subset\u003d[\u0027N_SOUSCRIP\u0027], keep\u003d\u0027first\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nunique_delegations \u003d labeled_data[\u0027delegation\u0027].nunique()\nprint(f\"Number of unique delegations: {unique_delegations}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nprint(labeled_data.head())\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\r\n###################\r\n# Import required libraries\r\nimport pandas as pd\r\nfrom xgboost import XGBClassifier\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import classification_report, roc_auc_score, fbeta_score\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\n\r\n# Features to retain\r\nfeatures_to_use \u003d [\r\n    \"puissance\", \"age_objet_assuree\", \"valeur_venale\", \"valeur_neuve\",\r\n    \"Charge_utile\", \"usage\", \"anciennete\", \"activite\", \"classe\",\r\n    \"delegation\", \"age_client\", \"civilite\", \"Risky\"\r\n]\r\n\r\n# Filter only the required features\r\nlabeled_data \u003d labeled_data[features_to_use]\r\n\r\n# Define categorical and numerical columns\r\ncategorical_cols \u003d [\"usage\", \"activite\", \"delegation\", \"civilite\"]\r\nnumerical_cols \u003d [\"puissance\", \"age_objet_assuree\", \"valeur_venale\", \"valeur_neuve\", \r\n                  \"Charge_utile\", \"anciennete\",\"classe\", \"age_client\"]\r\n\r\n# Ensure all numerical columns are floats or integers\r\nfor col in numerical_cols:\r\n    labeled_data[col] \u003d pd.to_numeric(labeled_data[col], errors\u003d\u0027coerce\u0027)\r\n\r\n# Ensure categorical columns are properly set to \u0027category\u0027 dtype\r\nfor col in categorical_cols:\r\n    labeled_data[col] \u003d labeled_data[col].astype(\u0027category\u0027)\r\n\r\n# Handle missing values: Drop rows with missing values in any critical column\r\nlabeled_data \u003d labeled_data.dropna(subset\u003dcategorical_cols + numerical_cols)\r\n\r\n# Define Features (X) and Target (y)\r\nX \u003d labeled_data[categorical_cols + numerical_cols]\r\ny \u003d (labeled_data[\"Risky\"] \u003d\u003d \"No\").astype(int)  # Binary: 1 for \u0027No\u0027, 0 for \u0027Yes\u0027\r\n\r\n# Split the dataset\r\nX_train, X_test, y_train, y_test \u003d train_test_split(\r\n    X, y, test_size\u003d0.2, stratify\u003dy, random_state\u003d42\r\n)\r\n\r\n# Calculate scale_pos_weight\r\nscale_pos_weight \u003d len(y_train[y_train \u003d\u003d 0]) / len(y_train[y_train \u003d\u003d 1])\r\n\r\n# Initialize XGBoost Model\r\nxgb \u003d XGBClassifier(\r\n    n_estimators\u003d100,\r\n    random_state\u003d42,\r\n    tree_method\u003d\u0027hist\u0027,\r\n    enable_categorical\u003dTrue,\r\n    scale_pos_weight\u003dscale_pos_weight\r\n)\r\n\r\n# Train the model\r\nxgb.fit(X_train, y_train)\r\n\r\n# Predictions\r\ny_pred \u003d xgb.predict(X_test)\r\ny_pred_prob \u003d xgb.predict_proba(X_test)[:, 1]\r\n\r\n# Evaluation metrics\r\nprint(\"Classification Report:\")\r\nprint(classification_report(y_test, y_pred, target_names\u003d[\"Yes (Risky)\", \"No (Non-Risky)\"]))\r\nprint(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_prob):.4f}\")\r\n\r\n\r\n# Adjust threshold\r\nthreshold \u003d 0.4\r\ny_pred_adjusted \u003d (y_pred_prob \u003e\u003d threshold).astype(int)\r\nprint(\"\\nClassification Report with Adjusted Threshold:\")\r\nprint(classification_report(y_test, y_pred_adjusted, target_names\u003d[\"Yes (Risky)\", \"No (Non-Risky)\"]))\r\nf2_score \u003d fbeta_score(y_test, y_pred_adjusted, beta\u003d2)\r\nprint(f\"F2-Score: {f2_score:.4f}\")\r\n\r\n\r\n###################\r\n\r\n#\"final\"\r\n# Classification Report:\r\n#                 precision    recall  f1-score   support\r\n\r\n#   Yes (Risky)       0.19      0.63      0.29       852\r\n# No (Non-Risky)       0.98      0.86      0.92     17271\r\n\r\n#       accuracy                           0.85     18123\r\n#      macro avg       0.58      0.75      0.60     18123\r\n#   weighted avg       0.94      0.85      0.89     18123\r\n\r\n# ROC-AUC Score: 0.8332\r\n\r\n# Classification Report with Adjusted Threshold:\r\n#                 precision    recall  f1-score   support\r\n\r\n#   Yes (Risky)       0.23      0.54      0.32       852\r\n# No (Non-Risky)       0.98      0.91      0.94     17271\r\n\r\n#       accuracy                           0.89     18123\r\n#      macro avg       0.60      0.72      0.63     18123\r\n#   weighted avg       0.94      0.89      0.91     18123\r\n\r\n# F2-Score: 0.9231\r\n\r\n\r\n\r\n#just like supposed\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nfrom sklearn.metrics import precision_recall_curve, fbeta_score\n\nprecision, recall, thresholds \u003d precision_recall_curve(y_test, y_pred_prob)\nf2_score \u003d fbeta_score(y_test, y_pred, beta\u003d2)\n\nprint(f\"F2-Score: {f2_score:.4f}\")\n#F2-Score: 0.2554,most features\n# F2-Score: 0.2888,selected oens only\n\n\n\n#F2-Score: 0.8854\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n############################################Final###########################\n##################run,is kept\n#trying class weights\n# Define class weights\nclass_weights \u003d {0: 1, 1: len(y) / (2 * sum(y))}  # Inverse class frequency as weight\n\n# Train the model with class weights\nxgb \u003d XGBClassifier(\n    n_estimators\u003d100,\n    random_state\u003d42,\n    tree_method\u003d\u0027hist\u0027,\n    enable_categorical\u003dTrue,\n    scale_pos_weight\u003dclass_weights[1]  # Weight for the minority class\n)\nxgb.fit(X_train, y_train)\n\n# Evaluate\ny_pred \u003d xgb.predict(X_test)\ny_pred_prob \u003d xgb.predict_proba(X_test)[:, 1]\nprint(\"Classification Report with Cost-Sensitive Learning:\")\nprint(classification_report(y_test, y_pred, target_names\u003d[\"Yes (Risky)\", \"No (Non-Risky)\"]))\nprint(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_prob):.4f}\")\n\n#\"final\"\n# Classification Report with Cost-Sensitive Learning:\n#                 precision    recall  f1-score   support\n\n#   Yes (Risky)       0.37      0.28      0.32       852\n# No (Non-Risky)       0.96      0.98      0.97     17271\n\n#       accuracy                           0.94     18123\n#      macro avg       0.67      0.63      0.64     18123\n#   weighted avg       0.94      0.94      0.94     18123\n\n# ROC-AUC Score: 0.8240"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nfrom sklearn.metrics import precision_recall_curve, fbeta_score\n\nprecision, recall, thresholds \u003d precision_recall_curve(y_test, y_pred_prob)\nf2_score \u003d fbeta_score(y_test, y_pred, beta\u003d2)\n\nprint(f\"F2-Score: {f2_score:.4f}\")\n#F2-Score: 0.9742\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\r\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\r\n\r\n# Generate confusion matrix with reversed axis\r\ncm \u003d confusion_matrix(y_test, y_pred, labels\u003d[1, 0])  # 1: No (Non-Risky), 0: Yes (Risky)\r\n\r\n# Display confusion matrix\r\ndisp \u003d ConfusionMatrixDisplay(confusion_matrix\u003dcm, display_labels\u003d[\"No (Non-Risky)\", \"Yes (Risky)\"])\r\ndisp.plot(cmap\u003d\"Blues\")\r\nplt.title(\"Confusion Matrix (Reversed Axes)\")\r\nplt.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nimport pickle\n# Save the model using pickle\nwith open(\"xgboost_final_model.pkl\", \"wb\") as file:\n    pickle.dump(xgb, file)\nprint(\"Model saved as \u0027file:///team5/data/xgboost_final_model.pkl\u0027\")"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n// another saving method\nmodel.save_model(\"file:///team5/data/xgboost_model.json\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n#model_path\u003d \"file:///team5/data/xgboost_final_model.pkl\"\nmodel_path\u003d \"/team5/data/models/xgboost_final_model.pkl\"\n# model \u003d pickle.load(open(model_path, \"rb\"))\n# print(model.keys())  # Check the keys in the dictionary\n# xgb_model \u003d model.get(\u0027model\u0027)  # Adjust based on the actual key name\n\n\ntry:\n     model \u003d pickle.load(open(model_path, \"rb\"))\nexcept FileNotFoundError:\n     st.error(\"TEST!Model file not found. Ensure \u0027xgboost_final_model.pkl\u0027 is in the correct directory.\")\nprint(\"Model type:\", type(model))\ntry:\n    example_input \u003d np.array([\n        [0,   # \u0027usage\u0027: Encoded value for \"VP\"\n        3,   # \u0027activite\u0027: Encoded value for \"RETRAITE\"\n        4,   # \u0027classe\u0027: Numerical or encoded value\n        15,  # \u0027delegation\u0027: Encoded index\n        1,   # \u0027civilite\u0027: Encoded value for \"Mme\"\n        6,   # \u0027puissance\u0027: Numerical value\n        5,   # \u0027age_objet_assuree\u0027: Numerical value\n        20000, # \u0027valeur_venale\u0027: Numerical value\n        25000, # \u0027valeur_neuve\u0027: Numerical value\n        1.5,  # \u0027Charge_utile\u0027: Numerical value\n        2,    # \u0027anciennete\u0027: Numerical value\n        45    # \u0027age_client\u0027: Numerical value\n        ]\n    ])\n\n    # Predict\n    prediction \u003d model.predict(example_input)\n    print(\"Prediction:\", prediction)\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n#1\u003d\u003erisky?"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n#model_path\u003d \"file:///team5/data/xgboost_final_model.pkl\"\nmodel_path\u003d \"/team5/data/models/xgboost_final_model.pkl\"\n# model \u003d pickle.load(open(model_path, \"rb\"))\n# print(model.keys())  # Check the keys in the dictionary\n# xgb_model \u003d model.get(\u0027model\u0027)  # Adjust based on the actual key name\n\n\ntry:\n     model \u003d pickle.load(open(model_path, \"rb\"))\nexcept FileNotFoundError:\n     st.error(\"TEST!Model file not found. Ensure \u0027xgboost_final_model.pkl\u0027 is in the correct directory.\")\nprint(\"Model type:\", type(model))\ntry:\n    example_input \u003d np.array([\n        [0,   # \u0027usage\u0027: Encoded value for \"VP\"\n        3,   # \u0027activite\u0027: Encoded value for \"RETRAITE\"\n        4,   # \u0027classe\u0027: Numerical or encoded value\n        15,  # \u0027delegation\u0027: Encoded index\n        1,   # \u0027civilite\u0027: Encoded value for \"Mme\"\n        7,   # \u0027puissance\u0027: Numerical value\n        5,   # \u0027age_objet_assuree\u0027: Numerical value\n        2000, # \u0027valeur_venale\u0027: Numerical value\n        25000, # \u0027valeur_neuve\u0027: Numerical value\n        1.5,  # \u0027Charge_utile\u0027: Numerical value\n        2,    # \u0027anciennete\u0027: Numerical value\n        18    # \u0027age_client\u0027: Numerical value\n        ]\n    ])\n\n    # Predict\n    prediction \u003d model.predict(example_input)\n    print(\"Prediction:\", prediction)\n\nexcept Exception as e:\n    print(f\"Error: {e}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# Get feature importances\nfeature_importances \u003d xgb.feature_importances_\nfeature_names \u003d categorical_cols + numerical_cols\n\n# Display feature importances\nprint(\"\\nFeature Importances:\")\nfor name, importance in zip(feature_names, feature_importances):\n    print(f\"Feature: {name}, Importance: {importance:.4f}\")\n\n# Visualize feature importances\nplt.figure(figsize\u003d(10, 6))\nplt.barh(feature_names, feature_importances, color\u003d\u0027skyblue\u0027)\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.title(\"Feature Importances\")\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n################################### def randomsearch\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBClassifier\nimport logging\n\n# Define parameter grid\nparam_grid \u003d {\n    \u0027max_depth\u0027: [3, 5, 7, 9],\n    \u0027min_child_weight\u0027: [1, 3, 5],\n    \u0027gamma\u0027: [0, 0.1, 0.2, 0.3],\n    \u0027subsample\u0027: [0.6, 0.8, 1.0],\n    \u0027colsample_bytree\u0027: [0.6, 0.8, 1.0],\n    \u0027learning_rate\u0027: [0.01, 0.1, 0.2],\n    \u0027n_estimators\u0027: [50, 100, 200],\n    \u0027scale_pos_weight\u0027: [1, len(y_train[y_train \u003d\u003d 0]) / len(y_train[y_train \u003d\u003d 1])],\n}\n\n# Initialize the model\nxgb \u003d XGBClassifier(tree_method\u003d\u0027hist\u0027, enable_categorical\u003dTrue, random_state\u003d42)\n\n# Set up RandomizedSearchCV\nrandom_search \u003d RandomizedSearchCV(\n    estimator\u003dxgb,\n    param_distributions\u003dparam_grid,\n    scoring\u003d\u0027roc_auc\u0027,\n    n_iter\u003d20,  # Number of combinations to try\n    cv\u003d2,       # 3-fold cross-validation\n    verbose\u003d1,\n    random_state\u003d42,\n    n_jobs\u003d-1,\n    )\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n#trying subset,here the  imbalance is kept\n###########################for finding params\nfrom sklearn.model_selection import train_test_split\n\n# Stratified sampling\nX_train_sub, X_test_sub, y_train_sub, y_test_sub \u003d train_test_split(\n    X, y, test_size\u003d0.95, random_state\u003d42, stratify\u003dy\n)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n# RandomizedSearchCV with subset data\nrandom_search.fit(X_train_sub, y_train_sub)\n\n# Best parameters\nbest_params \u003d random_search.best_params_\nprint(\"Best Parameters:\", best_params)\n\n# Fitting 2 folds for each of 20 candidates, totalling 40 fits\n# Best Parameters: {\u0027subsample\u0027: 0.8, \u0027scale_pos_weight\u0027: 1, \u0027n_estimators\u0027: 100, \u0027min_child_weight\u0027: 1, \u0027max_depth\u0027: 3, \u0027learning_rate\u0027: 0.01, \u0027gamma\u0027: 0, \u0027colsample_bytree\u0027: 1.0}"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nimport logging\nfrom sklearn.utils import parallel_backend\n#################################if trying ti find bestparas again,same as previous cell but with logs\n# Set up logging\nlogging.basicConfig(level\u003dlogging.INFO, format\u003d\u0027%(asctime)s - %(message)s\u0027)\nlogger \u003d logging.getLogger()\n\n# Use parallel_backend for better multiprocessing\nwith parallel_backend(\u0027threading\u0027):\n    logger.info(\"Starting RandomizedSearchCV...\")\n    random_search.fit(X_train_sub, y_train_sub)\n    logger.info(\"Finished RandomizedSearchCV.\")\n\n# Best parameters\nbest_params \u003d random_search.best_params_\nlogger.info(f\"Best Parameters: {best_params}\")\n\n#INFO:root:Starting RandomizedSearchCV...\n#Fitting 2 folds for each of 20 candidates, totalling 40 fits\n#INFO:root:Finished RandomizedSearchCV.\n#INFO:root:Best Parameters: {\u0027subsample\u0027: 0.8, \u0027scale_pos_weight\u0027: 65.33240863587022, \u0027n_estimators\u0027: 200, #\u0027min_child_weight\u0027: 1, \u0027max_depth\u0027: 9, \u0027learning_rate\u0027: 0.1, \u0027gamma\u0027: 0, \u0027colsample_bytree\u0027: 1.0}\n#yay...\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n########################################\nxgb_tuned \u003d XGBClassifier(\n    subsample\u003d0.8,\n    scale_pos_weight\u003d1,\n    n_estimators\u003d100,\n    min_child_weight\u003d5,\n    max_depth\u003d3,\n    learning_rate\u003d0.2,\n    gamma\u003d0.3,\n    colsample_bytree\u003d1.0,\n    tree_method\u003d\u0027hist\u0027,  # Use histogram-based method\n    enable_categorical\u003dTrue,\n    random_state\u003d42\n)\n\nxgb_tuned.fit(X_train, y_train)\ny_pred \u003d xgb_tuned.predict(X_test)\ny_pred_prob \u003d xgb_tuned.predict_proba(X_test)[:, 1]\n\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names\u003d[\"Yes (Risky)\", \"No (Non-Risky)\"]))\nprint(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_prob):.4f}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nfrom sklearn.metrics import fbeta_score\n\nf2_score \u003d fbeta_score(y_test, y_pred, beta\u003d2)\nprint(f\"F2-Score: {f2_score:.4f}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nimport joblib\n\n# Save the model\njoblib.dump(xgb_tuned, \u0027skeyenote-0.9.1/notebook/xgb_model.pkl\u0027)\n\n// xgb_loaded \u003d joblib.load(\u0027xgb_model.pkl\u0027)\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%python\n"
    }
  ]
}