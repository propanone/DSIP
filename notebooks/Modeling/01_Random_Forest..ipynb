{
  "metadata": {
    "name": "01_Random_Forest",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler}\nimport org.apache.spark.ml.classification.{RandomForestClassifier, RandomForestClassificationModel}\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.sql.functions._"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val labeledDf \u003d spark.read\n              .option(\"header\", \"true\")\n              .options(Map(\"inferSchema\" -\u003e \"true\", \"delimiter\" -\u003e \"\\t\"))\n              .csv(\"file:///team5/data/LabeledFile.csv\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//let\u0027s analyze categorical columns to see number of unique values\nprintln(\"Analyzing categorical features:\")\nval categoricalCols \u003d Array(\"marque\", \"usage\", \"Type_renouvellement_police\", \"fractionnement\", \"IsToutRisque\")\ncategoricalCols.foreach { colName \u003d\u003e\n  val uniqueCount \u003d labeledDf.select(colName).distinct().count()\n  println(s\"Column $colName has $uniqueCount unique values\")\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// let\u0027s prepare the categorical columns for encoding\nval numericCols \u003d Array(\"Prime\", \"Sinistre\", \"puissance\", \"age_objet_assuree\", \"valeur_venale\", \n                       \"valeur_neuve\", \"Charge_utile\", \"anciennete\", \"classe\", \"age_client\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Create string indexers for categorical columns\nval indexers \u003d categoricalCols.map { colName \u003d\u003e\n  new StringIndexer()\n    .setInputCol(colName)\n    .setOutputCol(colName + \"_indexed\")\n    .setHandleInvalid(\"keep\")\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// to delete\n\nval pipeline \u003d new Pipeline().setStages(indexers.toArray)\nval transformedDf \u003d pipeline.fit(labeledDf).transform(labeledDf)\n\ntransformedDf.select(\"usage\",\"usage_indexed\",\"Type_renouvellement_police\",\"Type_renouvellement_police_indexed\",\"fractionnement\",\"fractionnement_indexed\",\"IsToutRisque\",\"IsToutRisque_indexed\").distinct().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Create vector assembler for feature columns\nval assembler \u003d new VectorAssembler()\n  .setInputCols((numericCols ++ categoricalCols.map(_ + \"_indexed\")))\n  .setOutputCol(\"features\")\n  .setHandleInvalid(\"keep\")"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Create label indexer\nval labelIndexer \u003d new StringIndexer()\n  .setInputCol(\"Risky\")\n  .setOutputCol(\"label\")\n  .setHandleInvalid(\"keep\")"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Split the data\nval Array(trainingData, testData) \u003d labeledDf.randomSplit(Array(0.8, 0.2), seed \u003d 1234)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "trainingData.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Create Random Forest Classifier with increased maxBins\nval rf \u003d new RandomForestClassifier()\n  .setLabelCol(\"label\")\n  .setFeaturesCol(\"features\")\n  .setNumTrees(100)\n  .setMaxDepth(10)\n  .setMaxBins(200)  // Increased from 32 to 200\n  .setSeed(1234)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Create the pipeline\nval pipeline \u003d new Pipeline()\n  .setStages(indexers ++ Array(labelIndexer, assembler, rf))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Train model\nval model \u003d pipeline.fit(trainingData)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Make predictions on test data\nval predictions \u003d model.transform(testData)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Select example rows to display\npredictions.select(\"prediction\", \"label\", \"features\").show(5,false)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Evaluate model\nval evaluator \u003d new MulticlassClassificationEvaluator()\n  .setLabelCol(\"label\")\n  .setPredictionCol(\"prediction\")\n  .setMetricName(\"accuracy\")\n\nval accuracy \u003d evaluator.evaluate(predictions)\nprintln(s\"\\nAccuracy \u003d ${accuracy}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Calculate F1 Score\nval f1Evaluator \u003d new MulticlassClassificationEvaluator()\n  .setLabelCol(\"label\")\n  .setPredictionCol(\"prediction\")\n  .setMetricName(\"f1\")\n\nval f1Score \u003d f1Evaluator.evaluate(predictions)\nprintln(s\"F1 Score \u003d ${f1Score}\")\n\n// Calculate additional metrics for a more complete evaluation\nval precisionEvaluator \u003d f1Evaluator.setMetricName(\"weightedPrecision\")\nval recallEvaluator \u003d f1Evaluator.setMetricName(\"weightedRecall\")\n\nval precision \u003d precisionEvaluator.evaluate(predictions)\nval recall \u003d recallEvaluator.evaluate(predictions)\n\nprintln(\"\\nDetailed Metrics:\")\nprintln(f\"Precision \u003d ${precision}%.4f\")\nprintln(f\"Recall \u003d ${recall}%.4f\")\nprintln(f\"F1 Score \u003d ${f1Score}%.4f\")\n\n// Show confusion matrix\nprintln(\"\\nConfusion Matrix:\")\npredictions.groupBy(\"label\", \"prediction\")\n  .count()\n  .orderBy(\"label\", \"prediction\")\n  .show()"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Get the Random Forest model from the pipeline and calculate feature importance\nval rfModel \u003d model.stages.last.asInstanceOf[RandomForestClassificationModel]\nval featureImportances \u003d rfModel.featureImportances\n\n// Create a list of feature names (both numeric and categorical)\nval featureNames \u003d numericCols ++ categoricalCols.map(_ + \"_indexed\")\n\n// Print feature importances\nprintln(\"\\nFeature Importances:\")\nfeatureNames.zip(featureImportances.toArray).sortBy(-_._2).foreach { case (feature, importance) \u003d\u003e\n  println(f\"Feature: $feature, Importance: $importance%.4f\")\n}\n\n// Save feature importances to a DataFrame for better visualization\nval importanceDF \u003d spark.createDataFrame(\n  featureNames.zip(featureImportances.toArray).map { case (feature, importance) \u003d\u003e \n    (feature, importance)\n  }\n).toDF(\"feature\", \"importance\")\n  .orderBy($\"importance\".desc)\n\nimportanceDF.show(false)\n\n// Print model parameters\nprintln(\"\\nModel Parameters:\")\nprintln(s\"Number of trees: ${rfModel.getNumTrees}\")\nprintln(s\"Max depth: ${rfModel.getMaxDepth}\")\nprintln(s\"Max bins: ${rfModel.getMaxBins}\")"
    }
  ]
}