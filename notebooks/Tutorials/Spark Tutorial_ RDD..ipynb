{
  "metadata": {
    "name": "Spark Tutorial: RDD",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In this guide, we will discuss the basics of RDDs and DataFrames, and provide some examples\nof how to use them."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Spark RDD\n\nSpark RDD provides the user with fine-grained control over data transformations.\n\nRDDs are used when:\n- low level operations are needed\n- the data does not fit into tabular format"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Example wordcount\n\nIn this example, we will explore Spark RDD API through a _wordcount_ pipeline.\n\n- **Input**: text file\n- **Output**: csv file contains words and their count"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n\nwget https://raw.githubusercontent.com/apache/spark/refs/heads/master/README.md -O /tmp/SPARK_README.md"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val inputFile: String \u003d \"file:///tmp/SPARK_README.md\""
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n\nhead -n 5 /tmp/SPARK_README.md"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "spark"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val textRdd \u003d sc.textFile(inputFile)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val xs \u003d  List(1, 2, 5, 6, 3, 4)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "xs.map(x \u003d\u003e x * 2)\n    .filter(x \u003d\u003e x \u003e\u003d 10)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n# for x in xs:\n#     new_list.append(x * 2)\n    \n# list(map(lambda x: x * 2, xs))"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "textRdd.take(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val regex \u003d \"\"\"\\w+\"\"\".r"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "textRdd\n    .filter(line \u003d\u003e !line.isEmpty()) // filter out empty lines\n    .flatMap(line \u003d\u003e regex.findAllIn(line))"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// _ a filler variable"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val wordcountRdd \u003d textRdd\n    .filter(!_.isEmpty()) // filter out empty lines\n    .flatMap(line \u003d\u003e regex.findAllIn(line)) // extract words from line\n    .map(_.toLowerCase()) // normalize word\n    .map(word \u003d\u003e (word, 1))\n    .reduceByKey(_ + _)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "wordcountRdd.take(2)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### Convert Rdd to Dataframe"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val wordcountDF \u003d wordcountRdd\n    .filter(x \u003d\u003e x._2 \u003e 1)\n    .toDF(\"word\", \"count\")"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "wordcountDF\n    .orderBy(desc(\"count\"))\n    .show(5, false)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "wordcountDF\n    .write\n    .format(\"csv\") // parquet, orc, json, \u0027jdbc\u0027\n    .options(Map(\"header\" -\u003e \"true\", \"delimiter\" -\u003e \"\\t\"))\n    .mode(\"overwrite\")\n    .save(\"/tmp/example/wordcount\")"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%file\n\nls /tmp/example/wordcount"
    }
  ]
}