{
  "metadata": {
    "name": "README",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data is stored into HDFS"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%file\n\nls /user/majesteye/DS05_INSURANCE_DATASET/input\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nsc.hadoopConfiguration.set(\"fs.defaultFS\", \"hdfs://namenode:9000\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df \u003d spark.read\n    .format(\"csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .option(\"delimiter\", \",\")\n    .load(\"/user/majesteye/DS05_INSURANCE_DATASET/input/*.csv\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Spark Memory Management\n\nIf you are working with large datasets in Spark and performing multiple actions or transformations on the same DataFrame, it’s a good idea to **cache** it to improve performance.\n\n### How to Cache a DataFrame\n\nYou can cache a DataFrame using `.cache()`:\n\n```scala\ndf.cache()  // Cache the DataFrame to memory\n```\n\n### Persist\n\n`persist()` is more flexible than cache because it allows you to specify how and where to store the DataFrame, such as in memory, on disk, or a combination of both.\n\n```scala\nimport org.apache.spark.storage.StorageLevel\ndf.persist(StorageLevel.MEMORY_AND_DISK)\n```\n\n`MEMORY_ONLY`: Stores the DataFrame in memory only. If there is not enough memory, the DataFrame will not be cached and must be recomputed.\n`MEMORY_AND_DISK`: Stores the DataFrame in memory, but spills it to disk if there is not enough memory available.\n`DISK_ONLY`: Stores the DataFrame only on disk. This is useful if the data is too large to fit in memory.\n\n### Unpersist\n\n`unpersist()` is used to release the cached or persisted DataFrame from memory or disk. This helps in freeing up resources once the DataFrame is no longer needed.\n\n```scala\ndf.unpersist()\n```"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    }
  ]
}