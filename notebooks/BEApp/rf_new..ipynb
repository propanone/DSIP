{
  "metadata": {
    "name": "rf_new",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# Cell 1: Import Libraries\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.metrics import accuracy_score, classification_report\r\nimport pickle"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# Cell 2: Configuration\r\n# Define features and file paths\r\nNUMERIC_FEATURES \u003d [\r\n    \"puissance\", \"age_objet_assuree\", \"valeur_venale\", \"valeur_neuve\",\r\n    \"Charge_utile\", \"anciennete\", \"classe\", \"age_client\"\r\n]\r\n\r\nCATEGORICAL_FEATURES \u003d [\"usage\", \"activite\", \"delegation\", \"civilite\"]\r\n\r\n# File paths\r\nINPUT_FILE_PATH \u003d \u0027file:///team5/data/LabeledFile.csv\u0027  # Update this path\r\nMODEL_SAVE_PATH \u003d \u0027random_forest.pkl\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# Cell 3: Load and Clean Data\r\n# Load the data\r\ndata \u003d pd.read_csv(INPUT_FILE_PATH, delimiter\u003d\u0027\\t\u0027)\r\n\r\n# Sort and deduplicate\r\ndata \u003d data.sort_values(by\u003d[\u0027N_SOUSCRIP\u0027, \u0027year\u0027, \u0027Risky\u0027], ascending\u003d[True, True, False])\r\ndata \u003d data.drop_duplicates(subset\u003d[\u0027N_SOUSCRIP\u0027], keep\u003d\u0027first\u0027)\r\n\r\n# Remove rows with missing values\r\ndata \u003d data.dropna(subset\u003dNUMERIC_FEATURES + CATEGORICAL_FEATURES)\r\n\r\nprint(\"Data shape after cleaning:\", data.shape)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# Cell 4: Prepare Features\r\n# Initialize preprocessors\r\nscaler \u003d StandardScaler()\r\nlabel_encoders \u003d {}\r\n\r\n# Prepare features (X)\r\nX \u003d data[CATEGORICAL_FEATURES + NUMERIC_FEATURES].copy()\r\n\r\n# Encode categorical features\r\nfor col in CATEGORICAL_FEATURES:\r\n    label_encoders[col] \u003d LabelEncoder()\r\n    X[col] \u003d label_encoders[col].fit_transform(X[col].astype(str))\r\n\r\n# Scale numeric features\r\nX[NUMERIC_FEATURES] \u003d scaler.fit_transform(X[NUMERIC_FEATURES])\r\n\r\n# Prepare target (y)\r\ntarget_encoder \u003d LabelEncoder()\r\ny \u003d target_encoder.fit_transform(data[\u0027Risky\u0027])\r\n\r\nprint(\"Features shape:\", X.shape)\r\nprint(\"Target shape:\", y.shape)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# Cell 5: Train Model\r\n# Split the data\r\nX_train, X_test, y_train, y_test \u003d train_test_split(\r\n    X, y, test_size\u003d0.2, random_state\u003d1400\r\n)\r\n\r\n# Initialize and train the model\r\nrf \u003d RandomForestClassifier(n_estimators\u003d200, max_depth\u003d14, random_state\u003d1400)\r\nrf.fit(X_train, y_train)\r\n\r\nprint(\"Model training completed!\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# Cell 6: Evaluate Model\r\n# Make predictions\r\ny_pred \u003d rf.predict(X_test)\r\n\r\n# Print evaluation metrics\r\nprint(\"Model Evaluation Metrics:\")\r\nprint(\"-----------------------\")\r\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\r\nprint(\"\\nClassification Report:\")\r\nprint(classification_report(y_test, y_pred, target_names\u003d[\"Negative\", \"Positive\"]))\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# Cell 7: Save Model\r\n# Save the model and preprocessors\r\nmodel_artifacts \u003d {\r\n    \u0027model\u0027: rf,\r\n    \u0027label_encoders\u0027: label_encoders,\r\n    \u0027scaler\u0027: scaler,\r\n    \u0027target_encoder\u0027: target_encoder,\r\n    \u0027numeric_features\u0027: NUMERIC_FEATURES,\r\n    \u0027categorical_features\u0027: CATEGORICAL_FEATURES\r\n}\r\n\r\nwith open(MODEL_SAVE_PATH, \u0027wb\u0027) as file:\r\n    pickle.dump(model_artifacts, file)\r\n\r\nprint(f\"Model and preprocessors saved to {MODEL_SAVE_PATH}\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import os\r\n\r\n# Ensure the directory exists\r\nos.makedirs(\u0027/team5/data/models\u0027, exist_ok\u003dTrue)\r\n\r\n# Update the save path\r\nMODEL_SAVE_PATH \u003d \u0027/team5/data/models/random_forest.pkl\u0027\r\n\r\n# Save the model and preprocessors\r\nmodel_artifacts \u003d {\r\n    \u0027model\u0027: rf,\r\n    \u0027label_encoders\u0027: label_encoders,\r\n    \u0027scaler\u0027: scaler,\r\n    \u0027target_encoder\u0027: target_encoder,\r\n    \u0027numeric_features\u0027: NUMERIC_FEATURES,\r\n    \u0027categorical_features\u0027: CATEGORICAL_FEATURES\r\n}\r\n\r\nwith open(MODEL_SAVE_PATH, \u0027wb\u0027) as file:\r\n    pickle.dump(model_artifacts, file)\r\n\r\nprint(f\"Model and preprocessors saved to {MODEL_SAVE_PATH}\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# Cell 8 (Optional): Test Loading Model\r\n# Verify the saved model\r\nwith open(MODEL_SAVE_PATH, \u0027rb\u0027) as file:\r\n    loaded_artifacts \u003d pickle.load(file)\r\n\r\nprint(\"Available artifacts:\", loaded_artifacts.keys())\r\nprint(\"\\nModel loaded successfully!\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# Cell 9: Import Additional Libraries\r\nimport seaborn as sns\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.metrics import confusion_matrix, f1_score\r\n\r\n# Cell 10: Calculate and Plot Confusion Matrix\r\n# Calculate confusion matrix\r\ncm \u003d confusion_matrix(y_test, y_pred)\r\n\r\n# Create confusion matrix plot\r\nplt.figure(figsize\u003d(8, 6))\r\nsns.heatmap(cm, annot\u003dTrue, fmt\u003d\u0027d\u0027, cmap\u003d\u0027Blues\u0027,\r\n            xticklabels\u003d[\"Negative\", \"Positive\"],\r\n            yticklabels\u003d[\"Negative\", \"Positive\"])\r\nplt.title(\u0027Confusion Matrix\u0027)\r\nplt.xlabel(\u0027Predicted\u0027)\r\nplt.ylabel(\u0027Actual\u0027)\r\nplt.show()\r\n\r\n# Print confusion matrix interpretation\r\nprint(\"\\nConfusion Matrix Interpretation:\")\r\nprint(f\"True Negatives: {cm[0][0]}\")\r\nprint(f\"False Positives: {cm[0][1]}\")\r\nprint(f\"False Negatives: {cm[1][0]}\")\r\nprint(f\"True Positives: {cm[1][1]}\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# Cell 11: Calculate and Plot F1 Score\r\n# Calculate F1 score\r\nf1 \u003d f1_score(y_test, y_pred)\r\n\r\n# Create F1 score plot\r\nplt.figure(figsize\u003d(8, 4))\r\nplt.barh([\u0027F1 Score\u0027], [f1], color\u003d\u0027skyblue\u0027)\r\nplt.xlim(0, 1)\r\nplt.title(\u0027F1 Score\u0027)\r\nplt.xlabel(\u0027Score\u0027)\r\n\r\n# Add value annotation to the bar\r\nplt.text(f1, 0, f\u0027{f1:.3f}\u0027, \r\n         verticalalignment\u003d\u0027center\u0027,\r\n         fontweight\u003d\u0027bold\u0027)\r\n\r\nplt.tight_layout()\r\nplt.show()\r\n\r\nprint(f\"\\nF1 Score: {f1:.3f}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# Cell 12 (Optional): Detailed Metrics Comparison Plot\r\nmetrics \u003d {\r\n    \u0027F1 Score\u0027: f1_score(y_test, y_pred),\r\n    \u0027Precision\u0027: precision_score(y_test, y_pred),\r\n    \u0027Recall\u0027: recall_score(y_test, y_pred),\r\n    \u0027Accuracy\u0027: accuracy_score(y_test, y_pred)\r\n}\r\n\r\n# Create comparison plot\r\nplt.figure(figsize\u003d(10, 5))\r\ncolors \u003d [\u0027skyblue\u0027, \u0027lightgreen\u0027, \u0027lightcoral\u0027, \u0027lightsalmon\u0027]\r\nbars \u003d plt.barh(list(metrics.keys()), list(metrics.values()), color\u003dcolors)\r\nplt.xlim(0, 1)\r\nplt.title(\u0027Model Performance Metrics\u0027)\r\nplt.xlabel(\u0027Score\u0027)\r\n\r\n# Add value annotations to the bars\r\nfor bar in bars:\r\n    width \u003d bar.get_width()\r\n    plt.text(width, bar.get_y() + bar.get_height()/2,\r\n             f\u0027{width:.3f}\u0027,\r\n             ha\u003d\u0027left\u0027, va\u003d\u0027center\u0027, fontweight\u003d\u0027bold\u0027)\r\n\r\nplt.tight_layout()\r\nplt.show()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# Loop through each column and perform the group by, count, and sort\nfor col in numeric_cols:\n    print(f\"Processing column: {col}\")\n    result \u003d (\n        labeled_data.groupby(col)\n        .size()\n        .reset_index(name\u003d\"count\")\n        .sort_values(by\u003d\"count\", ascending\u003dFalse)\n    )\n    \n    # Display top 10 results for the current column\n    print(result.head(260))\n    print(\"\\n\" + \"\u003d\"*50 + \"\\n\")  # Separator for better readability"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# Add this code at the end of your notebook after training the model\r\nimport os\r\nimport joblib\r\nfrom sklearn.ensemble import RandomForestClassifier\r\n\r\n# Ensure the directory exists\r\nos.makedirs(\u0027/team5/data/models\u0027, exist_ok\u003dTrue)\r\n\r\n# Create a new model with the same parameters\r\nnew_rf \u003d RandomForestClassifier(\r\n    n_estimators\u003d200,\r\n    max_depth\u003d14,\r\n    random_state\u003d1400\r\n)\r\n\r\n# Fit the model with your training data\r\nnew_rf.fit(X_train, y_train)\r\n\r\n# Create the artifacts dictionary\r\nartifacts \u003d {\r\n    \u0027model\u0027: new_rf,\r\n    \u0027label_encoders\u0027: label_encoders,  # Assuming `label_encoders` is defined earlier in your code\r\n    \u0027scaler\u0027: scaler,                 # Assuming `scaler` is defined earlier in your code\r\n    \u0027numeric_features\u0027: NUMERIC_FEATURES,  # Replace with your actual numeric features list\r\n    \u0027categorical_features\u0027: CATEGORICAL_FEATURES  # Replace with your actual categorical features list\r\n}\r\n\r\n# Save using joblib (more reliable for scikit-learn objects)\r\njoblib.dump(artifacts, \u0027/team5/data/models/random_forest.joblib\u0027)\r\n\r\nprint(\"Model saved successfully!\")\r\n"
    }
  ]
}